{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, NamedTuple\n",
    "\n",
    "import chex\n",
    "from jumanji import specs, Environment\n",
    "from jumanji.env import State\n",
    "from jumanji.types import TimeStep, restart, termination, transition\n",
    "from jumanji.viewer import Viewer\n",
    "import jumanji\n",
    "from jax.tree_util import tree_map\n",
    "from chex import dataclass\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib\n",
    "from numpy.typing import NDArray\n",
    "from utils import idx_to_action_enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bin.game_interfaces_binding import Game, GameState, GameAction, GameOptions, ActionType, SubActionType, TileMask, Craftsman, MapState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProconState:\n",
    "    is_pond: chex.Array\n",
    "    is_castle: chex.Array\n",
    "    has_t1_wall: chex.Array\n",
    "    has_t2_wall: chex.Array\n",
    "    has_t1_craftsman: chex.Array\n",
    "    has_t2_craftsman: chex.Array\n",
    "    is_t1_close_territory: chex.Array\n",
    "    is_t2_close_territory: chex.Array\n",
    "    is_t1_open_territory: chex.Array\n",
    "    is_t2_open_territory: chex.Array\n",
    "\n",
    "    agents: chex.Array # num_of_agents*EnvAgent\n",
    "\n",
    "    current_turn: chex.Array # ()\n",
    "    remaining_turns: chex.Array # ()\n",
    "    is_t1_turn: chex.Array # ()\n",
    "    wall_coeff: chex.Array # ()\n",
    "    castle_coeff: chex.Array # ()\n",
    "    territory_coeff: chex.Array # ()\n",
    "\n",
    "    key: chex.PRNGKey # (2,)\n",
    "\n",
    "class ProconObservation(NamedTuple):\n",
    "    is_pond: chex.Array\n",
    "    is_castle: chex.Array\n",
    "    has_t1_wall: chex.Array\n",
    "    has_t2_wall: chex.Array\n",
    "    has_t1_craftsman: chex.Array\n",
    "    has_t2_craftsman: chex.Array\n",
    "    is_t1_close_territory: chex.Array\n",
    "    is_t2_close_territory: chex.Array\n",
    "    is_t1_open_territory: chex.Array\n",
    "    is_t2_open_territory: chex.Array\n",
    "\n",
    "    agents: chex.Array # num_of_agents*EnvAgent\n",
    "\n",
    "    current_turn: chex.Array # ()\n",
    "    remaining_turns: chex.Array # ()\n",
    "    is_t1_turn: chex.Array # ()\n",
    "    wall_coeff: chex.Array # ()\n",
    "    castle_coeff: chex.Array # ()\n",
    "    territory_coeff: chex.Array # ()\n",
    "\n",
    "class EnvAgent(NamedTuple):\n",
    "    x: chex.Array\n",
    "    y: chex.Array\n",
    "    id: chex.Array\n",
    "    is_t1: chex.Array\n",
    "\n",
    "class EnvAction(NamedTuple):\n",
    "    action: chex.Array # ()\n",
    "    craftsman_id: chex.Array # ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.animation\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.axes import Axes\n",
    "from numpy.typing import NDArray\n",
    "from jumanji.environments.commons.maze_utils.maze_rendering import MazeViewer\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    " \n",
    "class ScoreType(Enum):\n",
    "    T1_OPEN_TERRITORY = 0\n",
    "    T2_OPEN_TERRITORY = 1\n",
    "    T1_CLOSE_TERRITORY = 2\n",
    "    T2_CLOSE_TERRITORY = 3\n",
    "    T1_WALL = 4\n",
    "    T2_WALL = 5\n",
    "    T1_CASTLE = 6\n",
    "    T2_CASTLE = 7\n",
    "    T1 = 8\n",
    "    T2 = 9\n",
    "\n",
    "def calculate_score(state: ProconState):\n",
    "    width = state.is_pond.shape[1]\n",
    "    height = state.is_pond.shape[0]\n",
    "\n",
    "    res = {\n",
    "        ScoreType.T1_OPEN_TERRITORY: 0,\n",
    "        ScoreType.T2_OPEN_TERRITORY: 0,\n",
    "        ScoreType.T1_CLOSE_TERRITORY: 0,\n",
    "        ScoreType.T2_CLOSE_TERRITORY: 0,\n",
    "        ScoreType.T1_WALL: 0,\n",
    "        ScoreType.T2_WALL: 0,\n",
    "        ScoreType.T1_CASTLE: 0,\n",
    "        ScoreType.T2_CASTLE: 0,\n",
    "        ScoreType.T1: 0,\n",
    "        ScoreType.T2: 0,\n",
    "    }\n",
    "\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            if state.is_t1_close_territory[row, col] or state.is_t1_open_territory[row, col]:\n",
    "                if state.is_t1_close_territory[row, col]:\n",
    "                    res[ScoreType.T1_CLOSE_TERRITORY] += state.territory_coeff\n",
    "                else:\n",
    "                    res[ScoreType.T1_OPEN_TERRITORY] += state.territory_coeff\n",
    "                if state.is_castle[row, col]:\n",
    "                    res[ScoreType.T1_CASTLE] += state.castle_coeff\n",
    "            elif state.is_t2_close_territory[row, col] or state.is_t2_open_territory[row, col]:\n",
    "                if state.is_t2_close_territory[row, col]:\n",
    "                    res[ScoreType.T2_CLOSE_TERRITORY] += state.territory_coeff\n",
    "                else:\n",
    "                    res[ScoreType.T2_OPEN_TERRITORY] += state.territory_coeff\n",
    "                if state.is_castle[row, col]:\n",
    "                    res[ScoreType.T2_CASTLE] += state.castle_coeff\n",
    "            elif state.has_t1_wall[row, col]:\n",
    "                res[ScoreType.T1_WALL] += state.wall_coeff\n",
    "            elif state.has_t2_wall[row, col]:\n",
    "                res[ScoreType.T2_WALL] += state.wall_coeff\n",
    "    \n",
    "    res[ScoreType.T1] = res[ScoreType.T1_OPEN_TERRITORY] + res[ScoreType.T1_CLOSE_TERRITORY] + res[ScoreType.T1_WALL] + res[ScoreType.T1_CASTLE]\n",
    "    res[ScoreType.T2] = res[ScoreType.T2_OPEN_TERRITORY] + res[ScoreType.T2_CLOSE_TERRITORY] + res[ScoreType.T2_WALL] + res[ScoreType.T2_CASTLE]\n",
    "\n",
    "    return res\n",
    "\n",
    "%matplotlib inline\n",
    "class ProconViewer(MazeViewer):\n",
    "    COLORS = {\n",
    "        \"T1_WALL\": [0, 0.024, 0.671],\n",
    "        \"T1_CLOSE_TERRITORY\": [0.133, 0.161, 1],\n",
    "        \"T1_OPEN_TERRITORY\": [0.573, 0.62, 1],\n",
    "        \"T1_CRAFTSMAN\": [0.278, 0.922, 0.906],\n",
    "        \"T2_WALL\": [0.569, 0, 0],\n",
    "        \"T2_CLOSE_TERRITORY\": [1, 0, 0],\n",
    "        \"T2_OPEN_TERRITORY\": [1, 0.635, 0.635],\n",
    "        \"T2_CRAFTSMAN\": [0.922, 0.278, 0.58],\n",
    "        \"CASTLE\": [0.988, 0.91, 0.145],\n",
    "        \"POND\": [0,0,0],\n",
    "    }\n",
    "\n",
    "    def __init__(self, name: str, render_mode: str = \"human\") -> None:\n",
    "        \"\"\"\n",
    "        Viewer for the `Procon` environment.\n",
    "\n",
    "        Args:\n",
    "            name: the window name to be used when initialising the window.\n",
    "            render_mode: the mode used to render the environment. Must be one of:\n",
    "                - \"human\": render the environment on screen.\n",
    "                - \"rgb_array\": return a numpy array frame representing the environment.\n",
    "        \"\"\"\n",
    "        super().__init__(name, render_mode)\n",
    "\n",
    "        image_names = [\n",
    "            \"c1\",\n",
    "            \"c2\",\n",
    "        ]\n",
    "\n",
    "        self._images = {name: Image.open(f\"assets/{name}.png\").resize((8,8)) for name in image_names}\n",
    "        \n",
    "    def render_with_score(self, state: ProconState) -> Optional[NDArray]:\n",
    "        self._clear_display()\n",
    "        fig, ax = self._get_fig_ax()\n",
    "        ax.clear()\n",
    "        self._add_grid_image(state, ax)\n",
    "\n",
    "        score = calculate_score(state)\n",
    "        fig.suptitle(f\"Procon - Territory: {state.territory_coeff}, Wall: {state.wall_coeff}, Castle: {state.castle_coeff}, Turns: {state.current_turn} - {datetime.now().strftime('%H:%M:%S %d-%m-%Y')}\")\n",
    "        ax.set_title(f\"Turn {state.current_turn} - Team 1: {score[ScoreType.T1]}, Team 2: {score[ScoreType.T2]}\\nT1 - Close: {score[ScoreType.T1_CLOSE_TERRITORY]}, Open: {score[ScoreType.T1_OPEN_TERRITORY]}, Wall: {score[ScoreType.T1_WALL]}, Castle: {score[ScoreType.T1_CASTLE]}\\nT2 - Close: {score[ScoreType.T2_CLOSE_TERRITORY]}, Open: {score[ScoreType.T2_OPEN_TERRITORY]}, Wall: {score[ScoreType.T2_WALL]}, Castle: {score[ScoreType.T2_CASTLE]}\")\n",
    "        return self._display(fig)\n",
    "\n",
    "    def animate_with_score(\n",
    "        self,\n",
    "        states: Sequence[ProconState],\n",
    "        interval: int = 200,\n",
    "        save_path: Optional[str] = None,\n",
    "    ) -> matplotlib.animation.FuncAnimation:\n",
    "        fig, ax = plt.subplots(num=f\"{self._name}Animation\", figsize=self.FIGURE_SIZE)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "        fig.suptitle(f\"Procon - Territory: {states[0].territory_coeff}, Wall: {states[0].wall_coeff}, Castle: {states[0].castle_coeff}, Turns: {states[0].current_turn} - {datetime.now().strftime('%H:%M:%S %d-%m-%Y')}\")\n",
    "\n",
    "        def make_frame(state_index: int) -> None:\n",
    "            ax.clear()\n",
    "            state = states[state_index]\n",
    "            self._add_grid_image(state, ax)\n",
    "            score = calculate_score(state)\n",
    "            ax.set_title(f\"Turn {state.current_turn} - Team 1: {score[ScoreType.T1]}, Team 2: {score[ScoreType.T2]}\\nT1 - Close: {score[ScoreType.T1_CLOSE_TERRITORY]}, Open: {score[ScoreType.T1_OPEN_TERRITORY]}, Wall: {score[ScoreType.T1_WALL]}, Castle: {score[ScoreType.T1_CASTLE]}\\nT2 - Close: {score[ScoreType.T2_CLOSE_TERRITORY]}, Open: {score[ScoreType.T2_OPEN_TERRITORY]}, Wall: {score[ScoreType.T2_WALL]}, Castle: {score[ScoreType.T2_CASTLE]}\")\n",
    "\n",
    "        # Create the animation object.\n",
    "        self._animation = matplotlib.animation.FuncAnimation(\n",
    "            fig,\n",
    "            make_frame,\n",
    "            frames=len(states),\n",
    "            interval=interval,\n",
    "        )\n",
    "\n",
    "        # Save the animation as a gif.\n",
    "        if save_path:\n",
    "            self._animation.save(save_path)\n",
    "\n",
    "        return self._animation\n",
    "\n",
    "\n",
    "    def _create_grid_image(self, state: ProconState) -> NDArray:\n",
    "        img = np.ones((*state.has_t1_wall.shape, 3))\n",
    "\n",
    "        img[state.is_t1_open_territory] = self.COLORS[\"T1_OPEN_TERRITORY\"]\n",
    "        img[state.is_t2_open_territory] = self.COLORS[\"T2_OPEN_TERRITORY\"]\n",
    "        img[state.is_t1_close_territory] = self.COLORS[\"T1_CLOSE_TERRITORY\"]\n",
    "        img[state.is_t2_close_territory] = self.COLORS[\"T2_CLOSE_TERRITORY\"]\n",
    "\n",
    "        img[state.has_t1_wall] = self.COLORS[\"T1_WALL\"]\n",
    "        img[state.has_t2_wall] = self.COLORS[\"T2_WALL\"]\n",
    "\n",
    "        img[state.is_castle] = self.COLORS[\"CASTLE\"]\n",
    "        img[state.is_pond] = self.COLORS[\"POND\"]\n",
    "\n",
    "        # add a transparent layer\n",
    "        img = np.dstack([img, np.ones(img.shape[:2])])\n",
    "\n",
    "        # upscale array by 8 times\n",
    "        img = np.kron(img, np.ones((8, 8, 1)))\n",
    "\n",
    "        img = Image.fromarray((img * 255).astype('uint8'))\n",
    "\n",
    "        # paste the craftsman on top\n",
    "        for agent in state.agents:\n",
    "            if agent.is_t1:\n",
    "                img.paste(self._images[\"c1\"], (agent.x*8, agent.y*8), self._images[\"c1\"])\n",
    "            else:\n",
    "                img.paste(self._images[\"c2\"], (agent.x*8, agent.y*8), self._images[\"c2\"])\n",
    "        \n",
    "        img = np.array(img)\n",
    "\n",
    "        img = self._draw_black_frame_around(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def _draw_black_frame_around(self, img: NDArray) -> NDArray:\n",
    "        img = np.pad(img, ((8, 8), (8, 8), (0, 0)),mode='constant', constant_values=255)\n",
    "        img[:8, :, :3] = img[-8:, :, :3] = img[:, :8, :3] = img[:, -8:, :3] = 0\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:55:43.338539Z",
     "start_time": "2023-11-24T10:55:43.220493Z"
    }
   },
   "outputs": [],
   "source": [
    "max_possible_actions = 17\n",
    "\n",
    "class ProconJumanji(Environment[ProconState]):\n",
    "    def __init__(self, gameOptions: GameOptions, map: list[list[int]], craftsmen: list[Craftsman]):\n",
    "        self.max_turns = gameOptions.maxTurns\n",
    "        self.map_width = gameOptions.mapWidth\n",
    "        self.map_height = gameOptions.mapHeight\n",
    "\n",
    "        self.num_agents = len(craftsmen)\n",
    "\n",
    "        self.game = Game(gameOptions, map, craftsmen)\n",
    "        self.game_options = gameOptions\n",
    "\n",
    "        self._viewer = ProconViewer(\"Procon\", render_mode=\"human\")\n",
    "\n",
    "        self.initial_state = self._game_state_to_env_state(self.game.getCurrentState(), self.game.gameOptions)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"Procon(\\n\"\n",
    "            f\"\\tmap_width={self.map_width},\\n\"\n",
    "            f\"\\tmap_height={self.map_height},\\n\"\n",
    "            f\"\\tnum_agents={self.num_agents!r}, \\n\"\n",
    "            \")\"\n",
    "        )\n",
    "\n",
    "    def reset(self, key: chex.PRNGKey) -> Tuple[ProconState, TimeStep]:\n",
    "        state = self.initial_state\n",
    "        timestep = restart(observation=self._env_state_to_observation(state), extras=None)\n",
    "\n",
    "        return state, timestep\n",
    "        \n",
    "\n",
    "    def step(self, state: ProconState, actions: chex.Array) -> Tuple[ProconState, TimeStep[ProconObservation]]:\n",
    "        game_state = self._env_state_to_game_state(state)\n",
    "        game_actions = self._env_actions_to_game_actions(actions)\n",
    "        \n",
    "        next_game_state = game_state.applyActions(game_actions)\n",
    "\n",
    "        # print(actions)\n",
    "\n",
    "        # for game_action in game_actions:\n",
    "        #     print(game_action.craftsmanId, game_action.actionType, game_action.subActionType)\n",
    "\n",
    "        print(next_game_state.map.calcPoints(self.game.gameOptions, True), next_game_state.map.calcPoints(self.game.gameOptions, False))\n",
    "\n",
    "        next_env_state = self._game_state_to_env_state(next_game_state, self.game.gameOptions)\n",
    "\n",
    "        next_observation = self._env_state_to_observation(next_env_state)\n",
    "\n",
    "        t1_score_prev, t2_score_prev = game_state.map.calcPoints(self.game.gameOptions, True), game_state.map.calcPoints(self.game.gameOptions, False)\n",
    "        t1_score_next, t2_score_next = next_game_state.map.calcPoints(self.game.gameOptions, True), next_game_state.map.calcPoints(self.game.gameOptions, False)\n",
    "\n",
    "        reward = np.array([t1_score_next - t1_score_prev, t2_score_next - t2_score_prev])\n",
    "        done = game_state\n",
    "\n",
    "        # timestep = jax.lax.cond(\n",
    "        #     done,\n",
    "        #     lambda reward, observation, extras: termination(\n",
    "        #         reward=reward,\n",
    "        #         observation=observation,\n",
    "        #         extras=extras,\n",
    "        #     ),\n",
    "        #     lambda reward, observation, extras: transition(\n",
    "        #         reward=reward,\n",
    "        #         observation=observation,\n",
    "        #         extras=extras,\n",
    "        #     ),\n",
    "        #     reward,\n",
    "        #     next_observation,\n",
    "        #     None,\n",
    "        # )\n",
    "\n",
    "        timestep = termination(\n",
    "                reward=reward,\n",
    "                observation=next_observation,\n",
    "                extras=None,\n",
    "        ) if done else transition(\n",
    "            reward=reward,\n",
    "            observation=next_observation,\n",
    "            extras=None,\n",
    "        )\n",
    "\n",
    "        return next_env_state, timestep\n",
    "\n",
    "\n",
    "\n",
    "    def observation_spec(self) -> specs.Spec[ProconObservation]:\n",
    "        \"\"\"agent encoding: (x, y, id, is_t1)\"\"\"\n",
    "        def boolArrayOfMapSize(name: str):\n",
    "            return specs.BoundedArray(\n",
    "                shape=(self.map_height, self.map_width), \n",
    "                dtype=bool, \n",
    "                minimum=False, \n",
    "                maximum=True, \n",
    "                name=name\n",
    "            )\n",
    "\n",
    "        return specs.Spec(\n",
    "            ProconObservation,\n",
    "            \"ObservationSpec\",\n",
    "            is_pond=boolArrayOfMapSize(\"is_pond\"),\n",
    "            is_castle=boolArrayOfMapSize(\"is_castle\"),\n",
    "            has_t1_wall=boolArrayOfMapSize(\"has_t1_wall\"),\n",
    "            has_t2_wall=boolArrayOfMapSize(\"has_t2_wall\"),\n",
    "            has_t1_craftsman=boolArrayOfMapSize(\"has_t1_craftsman\"),\n",
    "            has_t2_craftsman=boolArrayOfMapSize(\"has_t2_craftsman\"),\n",
    "            is_t1_close_territory=boolArrayOfMapSize(\"is_t1_close_territory\"),\n",
    "            is_t2_close_territory=boolArrayOfMapSize(\"is_t2_close_territory\"),\n",
    "            is_t1_open_territory=boolArrayOfMapSize(\"is_t1_open_territory\"),\n",
    "            is_t2_open_territory=boolArrayOfMapSize(\"is_t2_open_territory\"),\n",
    "            \n",
    "            # agents=specs.MultiDiscreteArray(num_values=jnp.array([[self.map_width, self.map_height, self.num_agents, 2]]*self.num_agents), dtype=jnp.int32, name=\"agents\"),\n",
    "            agents=specs.MultiDiscreteArray(num_values=jnp.array([self.num_agents]), dtype=jnp.int32, name=\"agents\"), #TODO: remove this\n",
    "            current_turn=specs.BoundedArray((), dtype=jnp.int32, minimum=0, maximum=self.max_turns, name=\"current_turn\"),\n",
    "            remaining_turns=specs.BoundedArray((), dtype=jnp.int32, minimum=0, maximum=self.max_turns, name=\"remaining_turns\"),\n",
    "            is_t1_turn=specs.BoundedArray((), dtype=bool, minimum=False, maximum=True, name=\"is_t1_turn\"),\n",
    "            wall_coeff=specs.BoundedArray((), dtype=jnp.int32, minimum=1, maximum=100, name=\"wall_coeff\"),\n",
    "            castle_coeff=specs.BoundedArray((), dtype=jnp.int32, minimum=1, maximum=1000, name=\"castle_coeff\"),\n",
    "            territory_coeff=specs.BoundedArray((), dtype=jnp.int32, minimum=1, maximum=1000, name=\"territory_coeff\"),\n",
    "        )\n",
    "\n",
    "    def action_spec(self) -> specs.Spec:\n",
    "        # (action + craftsman_id) in an array so that their is an order\n",
    "        return specs.BoundedArray((25,25), dtype=jnp.int32, minimum=0, maximum=max_possible_actions, name=\"action\")\n",
    "    \n",
    "    def _game_state_to_env_state(self, game_state: GameState, game_options: GameOptions) -> ProconState:\n",
    "        map_state = game_state.map\n",
    "        map = np.array(map_state.tiles)\n",
    "\n",
    "        agents = [EnvAgent(craftsman.x, craftsman.y, craftsman.id, craftsman.isT1) for craftsman in game_state.craftsmen.values()]\n",
    "\n",
    "        state = ProconState(\n",
    "            is_pond=map & (1 << TileMask.POND.value) > 0,\n",
    "            is_castle=map & (1 << TileMask.CASTLE.value) > 0,\n",
    "            has_t1_wall=map & (1 << TileMask.T1_WALL.value) > 0,\n",
    "            has_t2_wall=map & (1 << TileMask.T2_WALL.value) > 0,\n",
    "            has_t1_craftsman=map & (1 << TileMask.T1_CRAFTSMAN.value) > 0,\n",
    "            has_t2_craftsman=map & (1 << TileMask.T2_CRAFTSMAN.value) > 0,\n",
    "            is_t1_close_territory=map & (1 << TileMask.T1_CLOSE_TERRITORY.value) > 0,\n",
    "            is_t2_close_territory=map & (1 << TileMask.T2_CLOSE_TERRITORY.value) > 0,\n",
    "            is_t1_open_territory=map & (1 << TileMask.T1_OPEN_TERRITORY.value) > 0,\n",
    "            is_t2_open_territory=map & (1 << TileMask.T2_OPEN_TERRITORY.value) > 0,\n",
    "\n",
    "            agents=agents,\n",
    "\n",
    "            current_turn=game_state.turn,\n",
    "            remaining_turns=self.max_turns - game_state.turn,\n",
    "\n",
    "            is_t1_turn=game_state.isT1Turn,\n",
    "            wall_coeff=game_options.wallCoeff,\n",
    "            castle_coeff=game_options.castleCoeff,\n",
    "            territory_coeff=game_options.territoryCoeff,\n",
    "            key=jax.random.PRNGKey(0), # TODO: fix this\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    def _env_state_to_game_state(self, env_state: ProconState) -> GameState:\n",
    "        go = self.game.gameOptions\n",
    "        map_state = MapState(go.mapWidth, go.mapHeight)\n",
    "        map = np.zeros_like(np.array(map_state.tiles), dtype=jnp.int32)\n",
    "\n",
    "        map |= env_state.is_pond << TileMask.POND.value\n",
    "        map |= env_state.is_castle << TileMask.CASTLE.value\n",
    "        map |= env_state.has_t1_wall << TileMask.T1_WALL.value\n",
    "        map |= env_state.has_t2_wall << TileMask.T2_WALL.value\n",
    "        map |= env_state.has_t1_craftsman << TileMask.T1_CRAFTSMAN.value\n",
    "        map |= env_state.has_t2_craftsman << TileMask.T2_CRAFTSMAN.value\n",
    "        map |= env_state.is_t1_close_territory << TileMask.T1_CLOSE_TERRITORY.value\n",
    "        map |= env_state.is_t2_close_territory << TileMask.T2_CLOSE_TERRITORY.value\n",
    "        map |= env_state.is_t1_open_territory << TileMask.T1_OPEN_TERRITORY.value\n",
    "        map |= env_state.is_t2_open_territory << TileMask.T2_OPEN_TERRITORY.value\n",
    "\n",
    "\n",
    "        map_state.tiles = map.tolist()\n",
    "\n",
    "        craftsmen = {int(craftsman.id): Craftsman(craftsman.id, craftsman.x, craftsman.y, craftsman.is_t1) for craftsman in env_state.agents}\n",
    "\n",
    "        game_state = GameState(map_state, craftsmen, env_state.current_turn, env_state.is_t1_turn)\n",
    "        return game_state\n",
    "\n",
    "\n",
    "    def _env_state_to_observation(self, env_state: ProconState) -> ProconObservation:\n",
    "        return ProconObservation(\n",
    "            is_pond=env_state.is_pond,\n",
    "            is_castle=env_state.is_castle,\n",
    "            has_t1_wall=env_state.has_t1_wall,\n",
    "            has_t2_wall=env_state.has_t2_wall,\n",
    "            has_t1_craftsman=env_state.has_t1_craftsman,\n",
    "            has_t2_craftsman=env_state.has_t2_craftsman,\n",
    "            is_t1_close_territory=env_state.is_t1_close_territory,\n",
    "            is_t2_close_territory=env_state.is_t2_close_territory,\n",
    "            is_t1_open_territory=env_state.is_t1_open_territory,\n",
    "            is_t2_open_territory=env_state.is_t2_open_territory,\n",
    "\n",
    "            agents=env_state.agents,\n",
    "\n",
    "            current_turn=env_state.current_turn,\n",
    "            remaining_turns=env_state.remaining_turns,\n",
    "\n",
    "            is_t1_turn=env_state.is_t1_turn,\n",
    "            wall_coeff=env_state.wall_coeff,\n",
    "            castle_coeff=env_state.castle_coeff,\n",
    "            territory_coeff=env_state.territory_coeff,\n",
    "        )\n",
    "\n",
    "    def _env_action_to_game_action(self, env_action: EnvAction) -> GameAction:\n",
    "        return GameAction(env_action.craftsman_id, *idx_to_action_enum(env_action.action))\n",
    "\n",
    "    def _env_actions_to_game_actions(self, env_actions: chex.Array) -> chex.Array:\n",
    "        return [self._env_action_to_game_action(env_action) for env_action in env_actions]\n",
    "\n",
    "\n",
    "    def render(self, state: ProconState) -> Optional[NDArray]:\n",
    "        return self._viewer.render_with_score(state)\n",
    "\n",
    "    def animate(\n",
    "        self,\n",
    "        states: Sequence[ProconState],\n",
    "        interval: int = 200,\n",
    "        save_path: Optional[str] = None,\n",
    "    ) -> matplotlib.animation.FuncAnimation:\n",
    "        return self._viewer.animate_with_score(states, interval, save_path)\n",
    "\n",
    "# ENV_ID = \"Procon-v0\"\n",
    "# if not ENV_ID in _REGISTRY:\n",
    "#     register(id=\"Procon-v0\", entry_point=\"procon:ProconJumanji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jumanji.environments.routing.lbf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Mava Helpful functions and types\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmava\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjax\u001b[39;00m \u001b[39mimport\u001b[39;00m merge_leading_dims\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmava\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjumanji\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     AgentIDWrapper,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     LogWrapper,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     ObservationGlobalState,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     RwareMultiAgentWithGlobalStateWrapper,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmava\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m ExperimentOutput, LearnerState, OptStates, Params, PPOTransition\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmava\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluator_setup\n",
      "File \u001b[0;32m/mnt/f/procon-2023/cpp-simulator/Procon2023/mava/mava/wrappers/jumanji.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjumanji\u001b[39;00m \u001b[39mimport\u001b[39;00m specs\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjumanji\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv\u001b[39;00m \u001b[39mimport\u001b[39;00m Environment\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjumanji\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvironments\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrouting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlbf\u001b[39;00m \u001b[39mimport\u001b[39;00m LevelBasedForaging\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjumanji\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvironments\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrouting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrobot_warehouse\u001b[39;00m \u001b[39mimport\u001b[39;00m RobotWarehouse\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjumanji\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m TimeStep\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jumanji.environments.routing.lbf'"
     ]
    }
   ],
   "source": [
    "from typing import Any, Callable, Dict, Sequence, Tuple\n",
    "from colorama import Fore, Style\n",
    "\n",
    "import optax\n",
    "from optax._src.base import OptState\n",
    "import chex\n",
    "import distrax\n",
    "import flax.linen as nn\n",
    "from flax import struct\n",
    "from flax.core.frozen_dict import FrozenDict\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# Env requirements\n",
    "import jumanji\n",
    "from jumanji.env import Environment\n",
    "from jumanji import specs\n",
    "from jumanji.wrappers import AutoResetWrapper\n",
    "\n",
    "# Mava Helpful functions and types\n",
    "from mava.utils.jax import merge_leading_dims\n",
    "from mava.wrappers.jumanji import (\n",
    "    AgentIDWrapper,\n",
    "    LogWrapper,\n",
    "    ObservationGlobalState,\n",
    "    RwareMultiAgentWithGlobalStateWrapper,\n",
    ")\n",
    "from mava.types import ExperimentOutput, LearnerState, OptStates, Params, PPOTransition\n",
    "from mava.evaluator import evaluator_setup\n",
    "\n",
    "# Plot requirements\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import time\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor Network.\"\"\"\n",
    "\n",
    "    action_dim: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, observation: ProconObservation) -> distrax.Categorical:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = jnp.stack(jnp.array([\n",
    "            observation.is_pond, \n",
    "            observation.is_castle, \n",
    "            observation.has_t1_wall, \n",
    "            observation.has_t2_wall, \n",
    "            observation.has_t1_craftsman, \n",
    "            observation.has_t2_craftsman, \n",
    "            observation.is_t1_close_territory, \n",
    "            observation.is_t2_close_territory, \n",
    "            observation.is_t1_open_territory, \n",
    "            observation.is_t2_open_territory,\n",
    "        ])).reshape(-1)\n",
    "        coeff_list = jnp.array([observation.wall_coeff, observation.castle_coeff, observation.territory_coeff]).reshape(-1)\n",
    "        x = jnp.concatenate([x, coeff_list])\n",
    "\n",
    "        actor_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "        actor_output = nn.relu(actor_output)\n",
    "        actor_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
    "            actor_output\n",
    "        )\n",
    "        actor_output = nn.relu(actor_output)\n",
    "        actor_output = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_output)\n",
    "\n",
    "        # masked_logits = jnp.where(\n",
    "        #     observation.action_mask,\n",
    "        #     actor_output,\n",
    "        #     jnp.finfo(jnp.float32).min,\n",
    "        # )\n",
    "        # actor_policy = distrax.Categorical(logits=masked_logits)\n",
    "\n",
    "        actor_policy = distrax.Categorical(logits=actor_output)\n",
    "\n",
    "        return actor_policy\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic Network.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, observation: ProconObservation) -> chex.Array:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = jnp.stack(jnp.array([\n",
    "            observation.is_pond, \n",
    "            observation.is_castle, \n",
    "            observation.has_t1_wall, \n",
    "            observation.has_t2_wall, \n",
    "            observation.has_t1_craftsman, \n",
    "            observation.has_t2_craftsman, \n",
    "            observation.is_t1_close_territory, \n",
    "            observation.is_t2_close_territory, \n",
    "            observation.is_t1_open_territory, \n",
    "            observation.is_t2_open_territory,\n",
    "        ])).reshape(-1)\n",
    "        coeff_list = jnp.array([observation.wall_coeff, observation.castle_coeff, observation.territory_coeff]).reshape(-1)\n",
    "        x = jnp.concatenate([x, coeff_list])\n",
    "\n",
    "        critic_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
    "            x\n",
    "        )\n",
    "        critic_output = nn.relu(critic_output)\n",
    "        critic_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
    "            critic_output\n",
    "        )\n",
    "        critic_output = nn.relu(critic_output)\n",
    "        critic_output = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic_output\n",
    "        )\n",
    "\n",
    "        return jnp.squeeze(critic_output, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def get_learner_fn(\n",
    "    env: jumanji.Environment,\n",
    "    apply_fns: Tuple[Callable, Callable],\n",
    "    update_fns: Tuple[Callable, Callable],\n",
    "    config: Dict,\n",
    ") -> Callable:\n",
    "    \"\"\"Get the learner function.\"\"\"\n",
    "\n",
    "    # Unpack apply and update functions.\n",
    "    actor_apply_fn, critic_apply_fn = apply_fns\n",
    "    actor_update_fn, critic_update_fn = update_fns\n",
    "\n",
    "    def _update_step(learner_state: LearnerState, _: Any) -> Tuple[LearnerState, Tuple]:\n",
    "        \"\"\"A single update of the network.\n",
    "\n",
    "        This function steps the environment and records the trajectory batch for\n",
    "        training. It then calculates advantages and targets based on the recorded\n",
    "        trajectory and updates the actor and critic networks based on the calculated\n",
    "        losses.\n",
    "\n",
    "        Args:\n",
    "            learner_state (NamedTuple):\n",
    "                - params (Params): The current model parameters.\n",
    "                - opt_states (OptStates): The current optimizer states.\n",
    "                - rng (PRNGKey): The random number generator state.\n",
    "                - env_state (State): The environment state.\n",
    "                - last_timestep (TimeStep): The last timestep in the current trajectory.\n",
    "            _ (Any): The current metrics info.\n",
    "        \"\"\"\n",
    "\n",
    "        def _env_step(learner_state: LearnerState, _: Any) -> Tuple[LearnerState, PPOTransition]:\n",
    "            \"\"\"Step the environment.\"\"\"\n",
    "            params, opt_states, rng, env_state, last_timestep = learner_state\n",
    "\n",
    "            # SELECT ACTION\n",
    "            rng, policy_rng = jax.random.split(rng)\n",
    "            print(params.actor_params, last_timestep.observation)\n",
    "            actor_policy = actor_apply_fn(params.actor_params, last_timestep.observation)\n",
    "            value = critic_apply_fn(params.critic_params, last_timestep.observation)\n",
    "            action = actor_policy.sample(seed=policy_rng)\n",
    "            log_prob = actor_policy.log_prob(action)\n",
    "\n",
    "            # STEP ENVIRONMENT\n",
    "            env_state, timestep = jax.vmap(env.step, in_axes=(0, 0))(env_state, action)\n",
    "\n",
    "            # LOG EPISODE METRICS\n",
    "            done, reward = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.repeat(x, config[\"num_agents\"]).reshape(config[\"num_envs\"], -1),\n",
    "                (timestep.last(), timestep.reward),\n",
    "            )\n",
    "            info = {\n",
    "                \"episode_return\": env_state.episode_return_info,\n",
    "                \"episode_length\": env_state.episode_length_info,\n",
    "            }\n",
    "\n",
    "            transition = PPOTransition(\n",
    "                done, action, value, reward, log_prob, last_timestep.observation, info\n",
    "            )\n",
    "            learner_state = LearnerState(params, opt_states, rng, env_state, timestep)\n",
    "            return learner_state, transition\n",
    "\n",
    "        # STEP ENVIRONMENT FOR ROLLOUT LENGTH\n",
    "        learner_state, traj_batch = jax.lax.scan(\n",
    "            _env_step, learner_state, None, config[\"rollout_length\"]\n",
    "        )\n",
    "\n",
    "        # CALCULATE ADVANTAGE\n",
    "        params, opt_states, rng, env_state, last_timestep = learner_state\n",
    "        last_val = critic_apply_fn(params.critic_params, last_timestep.observation)\n",
    "\n",
    "        def _calculate_gae(\n",
    "            traj_batch: PPOTransition, last_val: chex.Array\n",
    "        ) -> Tuple[chex.Array, chex.Array]:\n",
    "            \"\"\"Calculate the GAE.\"\"\"\n",
    "\n",
    "            def _get_advantages(gae_and_next_value: Tuple, transition: PPOTransition) -> Tuple:\n",
    "                \"\"\"Calculate the GAE for a single transition.\"\"\"\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward = (\n",
    "                    transition.done,\n",
    "                    transition.value,\n",
    "                    transition.reward,\n",
    "                )\n",
    "                delta = reward + config[\"gamma\"] * next_value * (1 - done) - value\n",
    "                gae = delta + config[\"gamma\"] * config[\"gae_lambda\"] * (1 - done) * gae\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return advantages, advantages + traj_batch.value\n",
    "\n",
    "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "        def _update_epoch(update_state: Tuple, _: Any) -> Tuple:\n",
    "            \"\"\"Update the network for a single epoch.\"\"\"\n",
    "\n",
    "            def _update_minibatch(train_state: Tuple, batch_info: Tuple) -> Tuple:\n",
    "                \"\"\"Update the network for a single minibatch.\"\"\"\n",
    "\n",
    "                # UNPACK TRAIN STATE AND BATCH INFO\n",
    "                params, opt_states = train_state\n",
    "                traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                def _actor_loss_fn(\n",
    "                    actor_params: FrozenDict,\n",
    "                    actor_opt_state: OptState,\n",
    "                    traj_batch: PPOTransition,\n",
    "                    gae: chex.Array,\n",
    "                ) -> Tuple:\n",
    "                    \"\"\"Calculate the actor loss.\"\"\"\n",
    "                    # RERUN NETWORK\n",
    "                    actor_policy = actor_apply_fn(actor_params, traj_batch.obs)\n",
    "                    log_prob = actor_policy.log_prob(traj_batch.action)\n",
    "\n",
    "                    # CALCULATE ACTOR LOSS\n",
    "                    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    loss_actor1 = ratio * gae\n",
    "                    loss_actor2 = (\n",
    "                        jnp.clip(\n",
    "                            ratio,\n",
    "                            1.0 - config[\"clip_eps\"],\n",
    "                            1.0 + config[\"clip_eps\"],\n",
    "                        )\n",
    "                        * gae\n",
    "                    )\n",
    "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                    loss_actor = loss_actor.mean()\n",
    "                    entropy = actor_policy.entropy().mean()\n",
    "\n",
    "                    total_loss_actor = loss_actor - config[\"ent_coef\"] * entropy\n",
    "                    return total_loss_actor, (loss_actor, entropy)\n",
    "\n",
    "                def _critic_loss_fn(\n",
    "                    critic_params: FrozenDict,\n",
    "                    critic_opt_state: OptState,\n",
    "                    traj_batch: PPOTransition,\n",
    "                    targets: chex.Array,\n",
    "                ) -> Tuple:\n",
    "                    \"\"\"Calculate the critic loss.\"\"\"\n",
    "                    # RERUN NETWORK\n",
    "                    value = critic_apply_fn(critic_params, traj_batch.obs)\n",
    "\n",
    "                    # CALCULATE VALUE LOSS\n",
    "                    value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
    "                        -config[\"clip_eps\"], config[\"clip_eps\"]\n",
    "                    )\n",
    "                    value_losses = jnp.square(value - targets)\n",
    "                    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "                    critic_total_loss = config[\"vf_coef\"] * value_loss\n",
    "                    return critic_total_loss, (value_loss)\n",
    "\n",
    "                # CALCULATE ACTOR LOSS\n",
    "                actor_grad_fn = jax.value_and_grad(_actor_loss_fn, has_aux=True)\n",
    "                actor_loss_info, actor_grads = actor_grad_fn(\n",
    "                    params.actor_params, opt_states.actor_opt_state, traj_batch, advantages\n",
    "                )\n",
    "\n",
    "                # CALCULATE CRITIC LOSS\n",
    "                critic_grad_fn = jax.value_and_grad(_critic_loss_fn, has_aux=True)\n",
    "                critic_loss_info, critic_grads = critic_grad_fn(\n",
    "                    params.critic_params, opt_states.critic_opt_state, traj_batch, targets\n",
    "                )\n",
    "\n",
    "                # Compute the parallel mean (pmean) over the batch.\n",
    "                # This calculation is inspired by the Anakin architecture demo notebook.\n",
    "                # available at https://tinyurl.com/26tdzs5x\n",
    "                # This pmean could be a regular mean as the batch axis is on the same device.\n",
    "                actor_grads, actor_loss_info = jax.lax.pmean(\n",
    "                    (actor_grads, actor_loss_info), axis_name=\"batch\"\n",
    "                )\n",
    "                # pmean over devices.\n",
    "                actor_grads, actor_loss_info = jax.lax.pmean(\n",
    "                    (actor_grads, actor_loss_info), axis_name=\"device\"\n",
    "                )\n",
    "\n",
    "                critic_grads, critic_loss_info = jax.lax.pmean(\n",
    "                    (critic_grads, critic_loss_info), axis_name=\"batch\"\n",
    "                )\n",
    "                # pmean over devices.\n",
    "                critic_grads, critic_loss_info = jax.lax.pmean(\n",
    "                    (critic_grads, critic_loss_info), axis_name=\"device\"\n",
    "                )\n",
    "\n",
    "                # UPDATE ACTOR PARAMS AND OPTIMISER STATE\n",
    "                actor_updates, actor_new_opt_state = actor_update_fn(\n",
    "                    actor_grads, opt_states.actor_opt_state\n",
    "                )\n",
    "                actor_new_params = optax.apply_updates(params.actor_params, actor_updates)\n",
    "\n",
    "                # UPDATE CRITIC PARAMS AND OPTIMISER STATE\n",
    "                critic_updates, critic_new_opt_state = critic_update_fn(\n",
    "                    critic_grads, opt_states.critic_opt_state\n",
    "                )\n",
    "                critic_new_params = optax.apply_updates(params.critic_params, critic_updates)\n",
    "\n",
    "                new_params = Params(actor_new_params, critic_new_params)\n",
    "                new_opt_state = OptStates(actor_new_opt_state, critic_new_opt_state)\n",
    "\n",
    "                # PACK LOSS INFO\n",
    "                total_loss = actor_loss_info[0] + critic_loss_info[0]\n",
    "                value_loss = critic_loss_info[1]\n",
    "                actor_loss = actor_loss_info[1][0]\n",
    "                entropy = actor_loss_info[1][1]\n",
    "                loss_info = (\n",
    "                    total_loss,\n",
    "                    (value_loss, actor_loss, entropy),\n",
    "                )\n",
    "\n",
    "                return (new_params, new_opt_state), loss_info\n",
    "\n",
    "            params, opt_states, traj_batch, advantages, targets, rng = update_state\n",
    "            rng, shuffle_rng = jax.random.split(rng)\n",
    "\n",
    "            # SHUFFLE MINIBATCHES\n",
    "            batch_size = config[\"rollout_length\"] * config[\"num_envs\"]\n",
    "            permutation = jax.random.permutation(shuffle_rng, batch_size)\n",
    "            batch = (traj_batch, advantages, targets)\n",
    "            batch = jax.tree_util.tree_map(lambda x: merge_leading_dims(x, 2), batch)\n",
    "            shuffled_batch = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "            )\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.reshape(x, [config[\"num_minibatches\"], -1] + list(x.shape[1:])),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "\n",
    "            # UPDATE MINIBATCHES\n",
    "            (params, opt_states), loss_info = jax.lax.scan(\n",
    "                _update_minibatch, (params, opt_states), minibatches\n",
    "            )\n",
    "\n",
    "            update_state = (params, opt_states, traj_batch, advantages, targets, rng)\n",
    "            return update_state, loss_info\n",
    "\n",
    "        update_state = (params, opt_states, traj_batch, advantages, targets, rng)\n",
    "\n",
    "        # UPDATE EPOCHS\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, config[\"ppo_epochs\"]\n",
    "        )\n",
    "\n",
    "        params, opt_states, traj_batch, advantages, targets, rng = update_state\n",
    "        learner_state = LearnerState(params, opt_states, rng, env_state, last_timestep)\n",
    "        metric = traj_batch.info\n",
    "        return learner_state, (metric, loss_info)\n",
    "\n",
    "    def learner_fn(learner_state: LearnerState) -> ExperimentOutput:\n",
    "        \"\"\"Learner function.\n",
    "\n",
    "        This function represents the learner, it updates the network parameters\n",
    "        by iteratively applying the `_update_step` function for a fixed number of\n",
    "        updates. The `_update_step` function is vectorized over a batch of inputs.\n",
    "\n",
    "        Args:\n",
    "            learner_state (NamedTuple):\n",
    "                - params (Params): The initial model parameters.\n",
    "                - opt_states (OptStates): The initial optimizer states.\n",
    "                - rng (chex.PRNGKey): The random number generator state.\n",
    "                - env_state (LogEnvState): The environment state.\n",
    "                - timesteps (TimeStep): The initial timestep in the initial trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        batched_update_step = jax.vmap(_update_step, in_axes=(0, None), axis_name=\"batch\")\n",
    "\n",
    "        learner_state, (metric, loss_info) = jax.lax.scan(\n",
    "            batched_update_step, learner_state, None, config[\"num_updates_per_eval\"]\n",
    "        )\n",
    "        total_loss, (value_loss, loss_actor, entropy) = loss_info\n",
    "        return ExperimentOutput(\n",
    "            learner_state=learner_state,\n",
    "            episodes_info=metric,\n",
    "            total_loss=total_loss,\n",
    "            value_loss=value_loss,\n",
    "            loss_actor=loss_actor,\n",
    "            entropy=entropy,\n",
    "        )\n",
    "\n",
    "    return learner_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def learner_setup(\n",
    "    env: ProconJumanji, rngs: chex.Array, config: Dict\n",
    ") -> Tuple[Callable, Actor, LearnerState]:\n",
    "    \"\"\"Initialise learner_fn, network, optimiser, environment and states.\"\"\"\n",
    "    # Get available TPU cores.\n",
    "    n_devices = len(jax.devices())\n",
    "\n",
    "    # Get number of actions and agents.\n",
    "    num_actions = 17\n",
    "    num_agents = env.action_spec().shape[0]\n",
    "    print(f\"num_agents: {num_agents}. Setting num_agents to config.\")\n",
    "    \n",
    "    config[\"num_agents\"] = num_agents\n",
    "\n",
    "    # PRNG keys.\n",
    "    rng, rng_p = rngs\n",
    "\n",
    "    # Define network and optimiser.\n",
    "    actor_network = Actor(25*25*num_actions)\n",
    "    critic_network = Critic()\n",
    "    actor_optim = optax.chain(\n",
    "        optax.clip_by_global_norm(config[\"max_grad_norm\"]),\n",
    "        optax.adam(config[\"actor_lr\"], eps=1e-5),\n",
    "    )\n",
    "    critic_optim = optax.chain(\n",
    "        optax.clip_by_global_norm(config[\"max_grad_norm\"]),\n",
    "        optax.adam(config[\"critic_lr\"], eps=1e-5),\n",
    "    )\n",
    "\n",
    "    # Initialise observation.\n",
    "    obs = env.observation_spec().generate_value()\n",
    "    # Select only obs for a single agent.\n",
    "    # init_x = ObservationGlobalState(\n",
    "    #     agents_view=obs.agents_view[0],\n",
    "    #     action_mask=obs.action_mask[0],\n",
    "    #     global_state=obs.global_state[0],\n",
    "    #     step_count=obs.current_turn,\n",
    "    # )\n",
    "    init_x = obs\n",
    "    init_x = jax.tree_util.tree_map(lambda x: x[None, ...], init_x)\n",
    "\n",
    "    # Initialise actor params and optimiser state.\n",
    "    actor_params = actor_network.init(rng_p, init_x)\n",
    "    actor_opt_state = actor_optim.init(actor_params)\n",
    "\n",
    "    # Initialise critic params and optimiser state.\n",
    "    critic_params = critic_network.init(rng_p, init_x)\n",
    "    critic_opt_state = critic_optim.init(critic_params)\n",
    "\n",
    "    # Vmap network apply function over number of agents.\n",
    "    vmapped_actor_network_apply_fn = jax.vmap(\n",
    "        actor_network.apply,\n",
    "        # in_axes=(None, 1),\n",
    "        out_axes=(1),\n",
    "    )\n",
    "    vmapped_critic_network_apply_fn = jax.vmap(\n",
    "        critic_network.apply,\n",
    "        # in_axes=(None, 1),\n",
    "        out_axes=(1),\n",
    "    )\n",
    "\n",
    "    # Pack apply and update functions.\n",
    "    apply_fns = (vmapped_actor_network_apply_fn, vmapped_critic_network_apply_fn)\n",
    "    update_fns = (actor_optim.update, critic_optim.update)\n",
    "\n",
    "    # Get batched iterated update and replicate it to pmap it over cores.\n",
    "    learn = get_learner_fn(env, apply_fns, update_fns, config)\n",
    "    learn = jax.pmap(learn, axis_name=\"device\")\n",
    "\n",
    "    # Broadcast params and optimiser state to cores and batch.\n",
    "    broadcast = lambda x: jnp.broadcast_to(x, (n_devices, config[\"update_batch_size\"]) + x.shape)\n",
    "    actor_params = jax.tree_map(broadcast, actor_params)\n",
    "    actor_opt_state = jax.tree_map(broadcast, actor_opt_state)\n",
    "    critic_params = jax.tree_map(broadcast, critic_params)\n",
    "    critic_opt_state = jax.tree_map(broadcast, critic_opt_state)\n",
    "\n",
    "    # Initialise environment states and timesteps.\n",
    "    rng, *env_rngs = jax.random.split(\n",
    "        rng, n_devices * config[\"update_batch_size\"] * config[\"num_envs\"] + 1\n",
    "    )\n",
    "    env_states, timesteps = jax.vmap(env.reset, in_axes=(0))(\n",
    "        jnp.stack(env_rngs),\n",
    "    )\n",
    "\n",
    "    # Split rngs for each core.\n",
    "    rng, *step_rngs = jax.random.split(rng, n_devices * config[\"update_batch_size\"] + 1)\n",
    "\n",
    "    # Add dimension to pmap over.\n",
    "    reshape_step_rngs = lambda x: x.reshape((n_devices, config[\"update_batch_size\"]) + x.shape[1:])\n",
    "    step_rngs = reshape_step_rngs(jnp.stack(step_rngs))\n",
    "    reshape_states = lambda x: x.reshape(\n",
    "        (n_devices, config[\"update_batch_size\"], config[\"num_envs\"]) + x.shape[1:]\n",
    "    )\n",
    "    env_states = jax.tree_util.tree_map(reshape_states, env_states)\n",
    "    timesteps = jax.tree_util.tree_map(reshape_states, timesteps)\n",
    "\n",
    "    params = Params(actor_params, critic_params)\n",
    "    opt_states = OptStates(actor_opt_state, critic_opt_state)\n",
    "\n",
    "    init_learner_state = LearnerState(params, opt_states, step_rngs, env_states, timesteps)\n",
    "    return learn, actor_network, init_learner_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def plot_performance(metrics, ep_returns, start_time):\n",
    "      plt.figure(figsize=(8, 4))\n",
    "      clear_output(wait=True)\n",
    "\n",
    "      ep_returns.append(metrics.episodes_info[\"episode_return\"].mean())\n",
    "      # Plot the data\n",
    "      plt.plot(np.linspace(0, (time.time()-start_time)/ 60.0, len(list(ep_returns))),list(ep_returns))\n",
    "      plt.xlabel('Run Time [Minutes]')\n",
    "      plt.ylabel('Episode Return')\n",
    "      plt.title(f'Robotic Warehouse with 4 Agents')\n",
    "      # Show the plot\n",
    "      plt.show()\n",
    "      return ep_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"actor_lr\": 2.5e-4,\n",
    "    \"critic_lr\": 2.5e-4,\n",
    "    \"update_batch_size\": 2,\n",
    "    \"rollout_length\": 128,\n",
    "    \"num_updates\": 150,\n",
    "    \"num_envs\": 512,\n",
    "    \"ppo_epochs\": 16,\n",
    "    \"num_minibatches\": 32,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_eps\": 0.2,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"env_name\": \"Procon-v0\",\n",
    "    \"num_eval_episodes\": 32,\n",
    "    \"num_evaluation\": 50,\n",
    "    \"evaluation_greedy\": False,\n",
    "    \"add_agent_id\": True,\n",
    "    \"seed\":42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "action_path = \"test-cases/match-259.txt\"\n",
    "map_path = \"test-cases/map-259-game-2.txt\"\n",
    "\n",
    "from utils import load_offline_game, load_offline_actions\n",
    "\n",
    "game_options, map, craftsmen, craftsman_strid_to_intid = load_offline_game(map_path)\n",
    "\n",
    "# actions = load_offline_actions(action_path, craftsman_strid_to_intid)\n",
    "\n",
    "# env = ProconJumanji(game_options, map, craftsmen)\n",
    "\n",
    "# random_key = jax.random.PRNGKey(0)\n",
    "# key1, key2 = jax.random.split(random_key)\n",
    "# batch_size = 1\n",
    "\n",
    "# keys = jax.random.split(key1, batch_size)\n",
    "# state, timestep = env.reset(keys[0])\n",
    "\n",
    "# def game_actions_to_env_actions(all_turns_actions) -> list[list[EnvAction]]:\n",
    "#     env_all_turns_actions: list[list[EnvAction]] = []\n",
    "#     for actions in all_turns_actions:\n",
    "#         turn_actions: list[EnvAction] = []\n",
    "#         for action in actions:\n",
    "#             turn_actions.append(EnvAction(action.subActionType.value, action.craftsmanId))\n",
    "#         env_all_turns_actions.append(turn_actions)\n",
    "            \n",
    "#     return env_all_turns_actions\n",
    "\n",
    "# env_actions = game_actions_to_env_actions(actions)\n",
    "\n",
    "# state_list = [state]\n",
    "\n",
    "# for i, actions_in_turn in enumerate(env_actions):\n",
    "#     # print(actions_in_turn)\n",
    "#     state, timestep = env.step(state, actions_in_turn)\n",
    "#     state_list.append(state)\n",
    "# env.animate(state_list, interval=200, save_path=f'records/test.gif')\n",
    "# env.render(state_list[-1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "env = ProconJumanji(game_options, map, craftsmen)\n",
    "env = AutoResetWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "eval_env = ProconJumanji(game_options, map, craftsmen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# this block is for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# PRNG keys.\n",
    "rng, rng_e, rng_p = jax.random.split(jax.random.PRNGKey(config[\"seed\"]), num=3)\n",
    "\n",
    "# Setup learner.\n",
    "learn, actor_network, learner_state = learner_setup(env, (rng, rng_p), config)\n",
    "\n",
    "# Setup evaluator.\n",
    "evaluator, absolute_metric_evaluator, (trained_params, eval_rngs) = evaluator_setup(\n",
    "        eval_env=eval_env,\n",
    "        rng_e=rng_e,\n",
    "        network=actor_network,\n",
    "        params=learner_state.params.actor_params,\n",
    "        config=config,\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate total timesteps.\n",
    "n_devices = len(jax.devices())\n",
    "print(jax.devices())\n",
    "\n",
    "config[\"num_updates_per_eval\"] = config[\"num_updates\"] // config[\"num_evaluation\"]\n",
    "steps_per_rollout = (\n",
    "    n_devices\n",
    "    * config[\"num_updates_per_eval\"]\n",
    "    * config[\"rollout_length\"]\n",
    "    * config[\"update_batch_size\"]\n",
    "    * config[\"num_envs\"]\n",
    ")\n",
    "\n",
    "# Run experiment for a total number of evaluations.\n",
    "ep_returns=[]\n",
    "     \n",
    "\n",
    "\n",
    "learn(learner_state)\n",
    "\n",
    "# Compile the evaluator function\n",
    "_ = evaluator(trained_params, eval_rngs)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'procon-new-server' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n procon-new-server ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "for i in range(config[\"num_evaluation\"]):\n",
    "    # Train.\n",
    "    learner_output = learn(learner_state)\n",
    "    jax.block_until_ready(learner_output)\n",
    "\n",
    "\n",
    "    # Prepare for evaluation.\n",
    "    trained_params = jax.tree_util.tree_map(\n",
    "            lambda x: x[:, 0, ...],\n",
    "            learner_output.learner_state.params.actor_params,  # Select only actor params\n",
    "    )\n",
    "    rng_e, *eval_rngs = jax.random.split(rng_e, n_devices + 1)\n",
    "    eval_rngs = jnp.stack(eval_rngs)\n",
    "    eval_rngs = eval_rngs.reshape(n_devices, -1)\n",
    "\n",
    "    # Evaluate.\n",
    "    evaluator_output = evaluator(trained_params, eval_rngs)\n",
    "    jax.block_until_ready(evaluator_output)\n",
    "    ep_returns=plot_performance(evaluator_output, ep_returns, start_time)\n",
    "\n",
    "    # Update runner state to continue training.\n",
    "    learner_state = learner_output.learner_state\n",
    "\n",
    "# Return trained params to be used for rendering or testing.\n",
    "trained_params= jax.tree_util.tree_map(\n",
    "    lambda x: x[0, 0, ...], learner_output.learner_state.params.actor_params\n",
    ")\n",
    "print(f\"{Fore.CYAN}{Style.BRIGHT}MAPPO experiment completed{Style.RESET_ALL}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procon-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
