{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import chex\n",
    "from jumanji import specs, Environment\n",
    "from jumanji.env import State\n",
    "from jumanji.types import TimeStep, restart, termination, transition\n",
    "from jumanji.viewer import Viewer\n",
    "import jumanji\n",
    "from jax.tree_util import tree_map\n",
    "from chex import dataclass\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bin.game_interfaces_binding import Game, GameState, GameAction, GameOptions, ActionType, SubActionType, TileMask, Craftsman, MapState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvPosition(NamedTuple):\n",
    "    x: chex.Array  \n",
    "    y: chex.Array \n",
    "\n",
    "class EnvAgent(NamedTuple):\n",
    "    position: EnvPosition\n",
    "    id: chex.Array\n",
    "    is_t1: chex.Array\n",
    "\n",
    "@dataclass\n",
    "class ProconState:\n",
    "    # map: chex.Array # (map_height, map_width) \n",
    "\n",
    "    is_pond: chex.Array\n",
    "    is_castle: chex.Array\n",
    "    has_t1_wall: chex.Array\n",
    "    has_t2_wall: chex.Array\n",
    "    has_t1_craftsman: chex.Array\n",
    "    has_t2_craftsman: chex.Array\n",
    "    is_t1_close_territory: chex.Array\n",
    "    is_t2_close_territory: chex.Array\n",
    "    is_t1_open_territory: chex.Array\n",
    "    is_t2_open_territory: chex.Array\n",
    "\n",
    "    agents: chex.Array # (num_agents:Agent)\n",
    "    current_turn: chex.Array # ()\n",
    "    remaining_turns: chex.Array # ()\n",
    "    is_t1_turn: chex.Array # ()\n",
    "    key: chex.PRNGKey # (2,)\n",
    "\n",
    "class EnvAction(NamedTuple):\n",
    "    action: chex.Array # ()\n",
    "    craftsman_id: chex.Array # ()\n",
    "\n",
    "class ProconObservation(NamedTuple):\n",
    "    # map: chex.Array # (map_height, map_width) \n",
    "\n",
    "    is_pond: chex.Array\n",
    "    is_castle: chex.Array\n",
    "    has_t1_wall: chex.Array\n",
    "    has_t2_wall: chex.Array\n",
    "    has_t1_craftsman: chex.Array\n",
    "    has_t2_craftsman: chex.Array\n",
    "    is_t1_close_territory: chex.Array\n",
    "    is_t2_close_territory: chex.Array\n",
    "    is_t1_open_territory: chex.Array\n",
    "    is_t2_open_territory: chex.Array\n",
    "\n",
    "    agents: chex.Array # (num_agents:Agent)\n",
    "    current_turn: chex.Array # ()\n",
    "    remaining_turns: chex.Array # ()\n",
    "    is_t1_turn: chex.Array # ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.animation\n",
    "import matplotlib.cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.axes import Axes\n",
    "from numpy.typing import NDArray\n",
    "from jumanji.environments.commons.maze_utils.maze_rendering import MazeViewer\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    " \n",
    "class ScoreType(Enum):\n",
    "    T1_OPEN_TERRITORY = 0\n",
    "    T2_OPEN_TERRITORY = 1\n",
    "    T1_CLOSE_TERRITORY = 2\n",
    "    T2_CLOSE_TERRITORY = 3\n",
    "    T1_WALL = 4\n",
    "    T2_WALL = 5\n",
    "    T1_CASTLE = 6\n",
    "    T2_CASTLE = 7\n",
    "    T1 = 8\n",
    "    T2 = 9\n",
    "\n",
    "def calculate_score(state: ProconState, game_options: GameOptions):\n",
    "    width = state.is_pond.shape[1]\n",
    "    height = state.is_pond.shape[0]\n",
    "\n",
    "    res = {\n",
    "        ScoreType.T1_OPEN_TERRITORY: 0,\n",
    "        ScoreType.T2_OPEN_TERRITORY: 0,\n",
    "        ScoreType.T1_CLOSE_TERRITORY: 0,\n",
    "        ScoreType.T2_CLOSE_TERRITORY: 0,\n",
    "        ScoreType.T1_WALL: 0,\n",
    "        ScoreType.T2_WALL: 0,\n",
    "        ScoreType.T1_CASTLE: 0,\n",
    "        ScoreType.T2_CASTLE: 0,\n",
    "        ScoreType.T1: 0,\n",
    "        ScoreType.T2: 0,\n",
    "    }\n",
    "\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            if state.is_t1_close_territory[row, col] or state.is_t1_open_territory[row, col]:\n",
    "                if state.is_t1_close_territory[row, col]:\n",
    "                    res[ScoreType.T1_CLOSE_TERRITORY] += game_options.territoryCoeff\n",
    "                else:\n",
    "                    res[ScoreType.T1_OPEN_TERRITORY] += game_options.territoryCoeff\n",
    "                if state.is_castle[row, col]:\n",
    "                    res[ScoreType.T1_CASTLE] += game_options.castleCoeff\n",
    "            elif state.is_t2_close_territory[row, col] or state.is_t2_open_territory[row, col]:\n",
    "                if state.is_t2_close_territory[row, col]:\n",
    "                    res[ScoreType.T2_CLOSE_TERRITORY] += game_options.territoryCoeff\n",
    "                else:\n",
    "                    res[ScoreType.T2_OPEN_TERRITORY] += game_options.territoryCoeff\n",
    "                if state.is_castle[row, col]:\n",
    "                    res[ScoreType.T2_CASTLE] += game_options.castleCoeff\n",
    "            elif state.has_t1_wall[row, col]:\n",
    "                res[ScoreType.T1_WALL] += game_options.wallCoeff\n",
    "            elif state.has_t2_wall[row, col]:\n",
    "                res[ScoreType.T2_WALL] += game_options.wallCoeff\n",
    "    \n",
    "    res[ScoreType.T1] = res[ScoreType.T1_OPEN_TERRITORY] + res[ScoreType.T1_CLOSE_TERRITORY] + res[ScoreType.T1_WALL] + res[ScoreType.T1_CASTLE]\n",
    "    res[ScoreType.T2] = res[ScoreType.T2_OPEN_TERRITORY] + res[ScoreType.T2_CLOSE_TERRITORY] + res[ScoreType.T2_WALL] + res[ScoreType.T2_CASTLE]\n",
    "\n",
    "    return res\n",
    "\n",
    "%matplotlib inline\n",
    "class ProconViewer(MazeViewer):\n",
    "    COLORS = {\n",
    "        \"T1_WALL\": [0, 0.024, 0.671],\n",
    "        \"T1_CLOSE_TERRITORY\": [0.133, 0.161, 1],\n",
    "        \"T1_OPEN_TERRITORY\": [0.573, 0.62, 1],\n",
    "        \"T1_CRAFTSMAN\": [0.278, 0.922, 0.906],\n",
    "        \"T2_WALL\": [0.569, 0, 0],\n",
    "        \"T2_CLOSE_TERRITORY\": [1, 0, 0],\n",
    "        \"T2_OPEN_TERRITORY\": [1, 0.635, 0.635],\n",
    "        \"T2_CRAFTSMAN\": [0.922, 0.278, 0.58],\n",
    "        \"CASTLE\": [0.988, 0.91, 0.145],\n",
    "        \"POND\": [0,0,0],\n",
    "    }\n",
    "\n",
    "    def __init__(self, name: str, render_mode: str = \"human\") -> None:\n",
    "        \"\"\"\n",
    "        Viewer for the `Procon` environment.\n",
    "\n",
    "        Args:\n",
    "            name: the window name to be used when initialising the window.\n",
    "            render_mode: the mode used to render the environment. Must be one of:\n",
    "                - \"human\": render the environment on screen.\n",
    "                - \"rgb_array\": return a numpy array frame representing the environment.\n",
    "        \"\"\"\n",
    "        super().__init__(name, render_mode)\n",
    "\n",
    "        image_names = [\n",
    "            \"c1\",\n",
    "            \"c2\",\n",
    "        ]\n",
    "\n",
    "        self._images = {name: Image.open(f\"assets/{name}.png\").resize((8,8)) for name in image_names}\n",
    "        \n",
    "    def render_with_score(self, state: ProconState, game_options: GameOptions) -> Optional[NDArray]:\n",
    "        self._clear_display()\n",
    "        fig, ax = self._get_fig_ax()\n",
    "        ax.clear()\n",
    "        self._add_grid_image(state, ax)\n",
    "\n",
    "        score = calculate_score(state, game_options)\n",
    "        fig.suptitle(f\"Procon - Territory: {game_options.territoryCoeff}, Wall: {game_options.wallCoeff}, Castle: {game_options.castleCoeff}, Turns: {game_options.maxTurns} - {datetime.now().strftime('%H:%M:%S %d-%m-%Y')}\")\n",
    "        ax.set_title(f\"Turn {state.current_turn} - Team 1: {score[ScoreType.T1]}, Team 2: {score[ScoreType.T2]}\\nT1 - Close: {score[ScoreType.T1_CLOSE_TERRITORY]}, Open: {score[ScoreType.T1_OPEN_TERRITORY]}, Wall: {score[ScoreType.T1_WALL]}, Castle: {score[ScoreType.T1_CASTLE]}\\nT2 - Close: {score[ScoreType.T2_CLOSE_TERRITORY]}, Open: {score[ScoreType.T2_OPEN_TERRITORY]}, Wall: {score[ScoreType.T2_WALL]}, Castle: {score[ScoreType.T2_CASTLE]}\")\n",
    "        return self._display(fig)\n",
    "\n",
    "    def animate_with_score(\n",
    "        self,\n",
    "        states: Sequence[ProconState],\n",
    "        game_options: GameOptions,\n",
    "        interval: int = 200,\n",
    "        save_path: Optional[str] = None,\n",
    "    ) -> matplotlib.animation.FuncAnimation:\n",
    "        fig, ax = plt.subplots(num=f\"{self._name}Animation\", figsize=self.FIGURE_SIZE)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "        fig.suptitle(f\"Procon - Territory: {game_options.territoryCoeff}, Wall: {game_options.wallCoeff}, Castle: {game_options.castleCoeff}, Turns: {game_options.maxTurns} - {datetime.now().strftime('%H:%M:%S %d-%m-%Y')}\")\n",
    "\n",
    "        def make_frame(state_index: int) -> None:\n",
    "            ax.clear()\n",
    "            state = states[state_index]\n",
    "            self._add_grid_image(state, ax)\n",
    "            score = calculate_score(state, game_options)\n",
    "            ax.set_title(f\"Turn {state.current_turn} - Team 1: {score[ScoreType.T1]}, Team 2: {score[ScoreType.T2]}\\nT1 - Close: {score[ScoreType.T1_CLOSE_TERRITORY]}, Open: {score[ScoreType.T1_OPEN_TERRITORY]}, Wall: {score[ScoreType.T1_WALL]}, Castle: {score[ScoreType.T1_CASTLE]}\\nT2 - Close: {score[ScoreType.T2_CLOSE_TERRITORY]}, Open: {score[ScoreType.T2_OPEN_TERRITORY]}, Wall: {score[ScoreType.T2_WALL]}, Castle: {score[ScoreType.T2_CASTLE]}\")\n",
    "\n",
    "        # Create the animation object.\n",
    "        self._animation = matplotlib.animation.FuncAnimation(\n",
    "            fig,\n",
    "            make_frame,\n",
    "            frames=len(states),\n",
    "            interval=interval,\n",
    "        )\n",
    "\n",
    "        # Save the animation as a gif.\n",
    "        if save_path:\n",
    "            self._animation.save(save_path)\n",
    "\n",
    "        return self._animation\n",
    "\n",
    "\n",
    "    def _create_grid_image(self, state: ProconState) -> NDArray:\n",
    "        img = np.ones((*state.has_t1_wall.shape, 3))\n",
    "\n",
    "        img[state.is_t1_open_territory] = self.COLORS[\"T1_OPEN_TERRITORY\"]\n",
    "        img[state.is_t2_open_territory] = self.COLORS[\"T2_OPEN_TERRITORY\"]\n",
    "        img[state.is_t1_close_territory] = self.COLORS[\"T1_CLOSE_TERRITORY\"]\n",
    "        img[state.is_t2_close_territory] = self.COLORS[\"T2_CLOSE_TERRITORY\"]\n",
    "\n",
    "        img[state.has_t1_wall] = self.COLORS[\"T1_WALL\"]\n",
    "        img[state.has_t2_wall] = self.COLORS[\"T2_WALL\"]\n",
    "\n",
    "        img[state.is_castle] = self.COLORS[\"CASTLE\"]\n",
    "        img[state.is_pond] = self.COLORS[\"POND\"]\n",
    "\n",
    "\n",
    "        # img[state.has_t1_craftsman] = self.COLORS[\"T1_CRAFTSMAN\"]\n",
    "        # img[state.has_t2_craftsman] = self.COLORS[\"T2_CRAFTSMAN\"]\n",
    "\n",
    "        # add a transparent layer\n",
    "        img = np.dstack([img, np.ones(img.shape[:2])])\n",
    "\n",
    "        # upscale array by 8 times\n",
    "        img = np.kron(img, np.ones((8, 8, 1)))\n",
    "\n",
    "        img = Image.fromarray((img * 255).astype('uint8'))\n",
    "\n",
    "        # paste the craftsman on top\n",
    "        for agent in state.agents:\n",
    "            if agent.is_t1:\n",
    "                img.paste(self._images[\"c1\"], (agent.position.x*8, agent.position.y*8), self._images[\"c1\"])\n",
    "            else:\n",
    "                img.paste(self._images[\"c2\"], (agent.position.x*8, agent.position.y*8), self._images[\"c2\"])\n",
    "        \n",
    "        img = np.array(img)\n",
    "\n",
    "        img = self._draw_black_frame_around(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def _draw_black_frame_around(self, img: NDArray) -> NDArray:\n",
    "        img = np.pad(img, ((8, 8), (8, 8), (0, 0)),mode='constant', constant_values=255)\n",
    "        img[:8, :, :3] = img[-8:, :, :3] = img[:, :8, :3] = img[:, -8:, :3] = 0\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxToActionEnum = [\n",
    "    [ActionType.MOVE, SubActionType.MOVE_UP],\n",
    "    [ActionType.MOVE, SubActionType.MOVE_DOWN],\n",
    "    [ActionType.MOVE, SubActionType.MOVE_LEFT],\n",
    "    [ActionType.MOVE, SubActionType.MOVE_RIGHT],\n",
    "    [ActionType.MOVE, SubActionType.MOVE_UP_LEFT],\n",
    "    [ActionType.MOVE, SubActionType.MOVE_UP_RIGHT],\n",
    "    [ActionType.MOVE, SubActionType.MOVE_DOWN_LEFT],\n",
    "    [ActionType.MOVE, SubActionType.MOVE_DOWN_RIGHT],\n",
    "    [ActionType.BUILD, SubActionType.BUILD_UP],\n",
    "    [ActionType.BUILD, SubActionType.BUILD_DOWN],\n",
    "    [ActionType.BUILD, SubActionType.BUILD_LEFT],\n",
    "    [ActionType.BUILD, SubActionType.BUILD_RIGHT],\n",
    "    [ActionType.DESTROY, SubActionType.DESTROY_UP],\n",
    "    [ActionType.DESTROY, SubActionType.DESTROY_DOWN],\n",
    "    [ActionType.DESTROY, SubActionType.DESTROY_LEFT],\n",
    "    [ActionType.DESTROY, SubActionType.DESTROY_RIGHT],\n",
    "    [ActionType.STAY, SubActionType.STAY],\n",
    "]\n",
    "\n",
    "def map_tile_to_bitmask(tile):\n",
    "    if tile == 1:\n",
    "        return 1 << TileMask.POND.value\n",
    "    elif tile == 2:\n",
    "        return 1 << TileMask.CASTLE.value\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:55:43.338539Z",
     "start_time": "2023-11-24T10:55:43.220493Z"
    }
   },
   "outputs": [],
   "source": [
    "from jumanji import register\n",
    "from jumanji.registration import _REGISTRY\n",
    "\n",
    "max_possible_actions = 17\n",
    "\n",
    "class ProconJumanji(Environment[ProconState]):\n",
    "    def __init__(self, gameOptions: GameOptions, map, craftsmen):\n",
    "        self.max_turns = gameOptions.maxTurns\n",
    "        self.map_width = gameOptions.mapWidth\n",
    "        self.map_height = gameOptions.mapHeight\n",
    "\n",
    "        self.num_agents = len(craftsmen)\n",
    "\n",
    "        self.game = Game(gameOptions, map, craftsmen)\n",
    "        self.game_options = gameOptions\n",
    "\n",
    "        self._viewer = ProconViewer(\"Procon\", render_mode=\"human\")\n",
    "\n",
    "        self.initial_state = self._game_state_to_env_state(self.game.getCurrentState())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"Procon(\\n\"\n",
    "            f\"\\tmap_width={self.map_width},\\n\"\n",
    "            f\"\\tmap_height={self.map_height},\\n\"\n",
    "            f\"\\tnum_agents={self.num_agents!r}, \\n\"\n",
    "            \")\"\n",
    "        )\n",
    "\n",
    "    def reset(self, key: chex.PRNGKey) -> Tuple[ProconState, TimeStep]:\n",
    "        state = self.initial_state\n",
    "        timestep = restart(observation=self._env_state_to_observation(state), extras=None)\n",
    "\n",
    "        return state, timestep\n",
    "        \n",
    "\n",
    "    def step(self, state: ProconState, actions: chex.Array) -> Tuple[ProconState, TimeStep[ProconObservation]]:\n",
    "        game_state = self._env_state_to_game_state(state)\n",
    "        game_actions = self._env_actions_to_game_actions(actions)\n",
    "        \n",
    "        next_game_state = game_state.applyActions(game_actions)\n",
    "\n",
    "        print(actions)\n",
    "\n",
    "        for game_action in game_actions:\n",
    "            print(game_action.craftsmanId, game_action.actionType, game_action.subActionType)\n",
    "\n",
    "        print(next_game_state.map.calcPoints(self.game.gameOptions, True), next_game_state.map.calcPoints(self.game.gameOptions, False))\n",
    "\n",
    "        next_env_state = self._game_state_to_env_state(next_game_state)\n",
    "\n",
    "        next_observation = self._env_state_to_observation(next_env_state)\n",
    "\n",
    "        t1_score_prev, t2_score_prev = game_state.map.calcPoints(self.game.gameOptions, True), game_state.map.calcPoints(self.game.gameOptions, False)\n",
    "        t1_score_next, t2_score_next = next_game_state.map.calcPoints(self.game.gameOptions, True), next_game_state.map.calcPoints(self.game.gameOptions, False)\n",
    "\n",
    "        reward = np.array([t1_score_next - t1_score_prev, t2_score_next - t2_score_prev])\n",
    "        done = game_state.turn >= self.max_turns\n",
    "\n",
    "        # timestep = jax.lax.cond(\n",
    "        #     done,\n",
    "        #     lambda reward, observation, extras: termination(\n",
    "        #         reward=reward,\n",
    "        #         observation=observation,\n",
    "        #         extras=extras,\n",
    "        #     ),\n",
    "        #     lambda reward, observation, extras: transition(\n",
    "        #         reward=reward,\n",
    "        #         observation=observation,\n",
    "        #         extras=extras,\n",
    "        #     ),\n",
    "        #     reward,\n",
    "        #     next_observation,\n",
    "        #     None,\n",
    "        # )\n",
    "\n",
    "        timestep = termination(\n",
    "                reward=reward,\n",
    "                observation=next_observation,\n",
    "                extras=None,\n",
    "        ) if done else transition(\n",
    "            reward=reward,\n",
    "            observation=next_observation,\n",
    "            extras=None,\n",
    "        )\n",
    "\n",
    "        return next_env_state, timestep\n",
    "\n",
    "\n",
    "\n",
    "    def observation_spec(self) -> specs.Spec[ProconObservation]:\n",
    "        \"\"\"agent encoding: (x, y, id, is_t1)\"\"\"\n",
    "        def boolArrayOfMapSize(name: str):\n",
    "            return specs.BoundedArray(\n",
    "                shape=(self.map_height, self.map_width), \n",
    "                dtype=bool, \n",
    "                minimum=False, \n",
    "                maximum=True, \n",
    "                name=name\n",
    "            )\n",
    "\n",
    "        return specs.Spec(\n",
    "            ProconObservation,\n",
    "            \"ObservationSpec\",\n",
    "            is_pond=boolArrayOfMapSize(\"is_pond\"),\n",
    "            is_castle=boolArrayOfMapSize(\"is_castle\"),\n",
    "            has_t1_wall=boolArrayOfMapSize(\"has_t1_wall\"),\n",
    "            has_t2_wall=boolArrayOfMapSize(\"has_t2_wall\"),\n",
    "            has_t1_craftsman=boolArrayOfMapSize(\"has_t1_craftsman\"),\n",
    "            has_t2_craftsman=boolArrayOfMapSize(\"has_t2_craftsman\"),\n",
    "            is_t1_close_territory=boolArrayOfMapSize(\"is_t1_close_territory\"),\n",
    "            is_t2_close_territory=boolArrayOfMapSize(\"is_t2_close_territory\"),\n",
    "            is_t1_open_territory=boolArrayOfMapSize(\"is_t1_open_territory\"),\n",
    "            is_t2_open_territory=boolArrayOfMapSize(\"is_t2_open_territory\"),\n",
    "            \n",
    "            agents=specs.Array((self.num_agents, 4), dtype=jnp.int32, name=\"agents\"),\n",
    "            current_turn=specs.BoundedArray((), dtype=jnp.int32, minimum=0, maximum=self.max_turns, name=\"current_turn\"),\n",
    "            remaining_turns=specs.BoundedArray((), dtype=jnp.int32, minimum=0, maximum=self.max_turns, name=\"remaining_turns\"),\n",
    "            is_t1_turn=specs.BoundedArray((), dtype=bool, minimum=False, maximum=True, name=\"is_t1_turn\"),\n",
    "        )\n",
    "\n",
    "    def action_spec(self) -> specs.Spec:\n",
    "        # (action + craftsman_id) in an array so that their is an order\n",
    "        return specs.MultiDiscreteArray(num_values=jnp.array([[max_possible_actions, self.num_agents]]*self.num_agents, dtype=jnp.int32), name=\"action\")\n",
    "    \n",
    "    def _game_state_to_env_state(self, game_state: GameState) -> ProconState:\n",
    "        map_state = game_state.map\n",
    "        map = np.array(map_state.tiles)\n",
    "\n",
    "        agents = [EnvAgent(EnvPosition(craftsman.x, craftsman.y), craftsman.id, craftsman.isT1) for (id, craftsman) in game_state.craftsmen.items()]\n",
    "\n",
    "        state = ProconState(\n",
    "            is_pond=map & (1 << TileMask.POND.value) > 0,\n",
    "            is_castle=map & (1 << TileMask.CASTLE.value) > 0,\n",
    "            has_t1_wall=map & (1 << TileMask.T1_WALL.value) > 0,\n",
    "            has_t2_wall=map & (1 << TileMask.T2_WALL.value) > 0,\n",
    "            has_t1_craftsman=map & (1 << TileMask.T1_CRAFTSMAN.value) > 0,\n",
    "            has_t2_craftsman=map & (1 << TileMask.T2_CRAFTSMAN.value) > 0,\n",
    "            is_t1_close_territory=map & (1 << TileMask.T1_CLOSE_TERRITORY.value) > 0,\n",
    "            is_t2_close_territory=map & (1 << TileMask.T2_CLOSE_TERRITORY.value) > 0,\n",
    "            is_t1_open_territory=map & (1 << TileMask.T1_OPEN_TERRITORY.value) > 0,\n",
    "            is_t2_open_territory=map & (1 << TileMask.T2_OPEN_TERRITORY.value) > 0,\n",
    "\n",
    "            agents=agents,\n",
    "            current_turn=game_state.turn,\n",
    "            remaining_turns=self.max_turns - game_state.turn,\n",
    "            is_t1_turn=game_state.isT1Turn,\n",
    "            key=jax.random.PRNGKey(0), # TODO: fix this\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    def _env_state_to_game_state(self, env_state: ProconState) -> GameState:\n",
    "        go = self.game.gameOptions\n",
    "        map_state = MapState(go.mapWidth, go.mapHeight)\n",
    "        map = np.zeros_like(np.array(map_state.tiles), dtype=jnp.int32)\n",
    "\n",
    "        map |= env_state.is_pond << TileMask.POND.value\n",
    "        map |= env_state.is_castle << TileMask.CASTLE.value\n",
    "        map |= env_state.has_t1_wall << TileMask.T1_WALL.value\n",
    "        map |= env_state.has_t2_wall << TileMask.T2_WALL.value\n",
    "        map |= env_state.has_t1_craftsman << TileMask.T1_CRAFTSMAN.value\n",
    "        map |= env_state.has_t2_craftsman << TileMask.T2_CRAFTSMAN.value\n",
    "        map |= env_state.is_t1_close_territory << TileMask.T1_CLOSE_TERRITORY.value\n",
    "        map |= env_state.is_t2_close_territory << TileMask.T2_CLOSE_TERRITORY.value\n",
    "        map |= env_state.is_t1_open_territory << TileMask.T1_OPEN_TERRITORY.value\n",
    "        map |= env_state.is_t2_open_territory << TileMask.T2_OPEN_TERRITORY.value\n",
    "\n",
    "\n",
    "        map_state.tiles = map.tolist()\n",
    "\n",
    "        craftsmen = {int(craftsman.id): Craftsman(craftsman.id, craftsman.position.x, craftsman.position.y, craftsman.is_t1) for craftsman in env_state.agents}\n",
    "\n",
    "        game_state = GameState(map_state, craftsmen, env_state.current_turn, env_state.is_t1_turn)\n",
    "        return game_state\n",
    "\n",
    "\n",
    "    def _env_state_to_observation(self, env_state: ProconState) -> ProconObservation:\n",
    "        return ProconObservation(\n",
    "            is_pond=env_state.is_pond,\n",
    "            is_castle=env_state.is_castle,\n",
    "            has_t1_wall=env_state.has_t1_wall,\n",
    "            has_t2_wall=env_state.has_t2_wall,\n",
    "            has_t1_craftsman=env_state.has_t1_craftsman,\n",
    "            has_t2_craftsman=env_state.has_t2_craftsman,\n",
    "            is_t1_close_territory=env_state.is_t1_close_territory,\n",
    "            is_t2_close_territory=env_state.is_t2_close_territory,\n",
    "            is_t1_open_territory=env_state.is_t1_open_territory,\n",
    "            is_t2_open_territory=env_state.is_t2_open_territory,\n",
    "            agents=env_state.agents,\n",
    "            current_turn=env_state.current_turn,\n",
    "            remaining_turns=env_state.remaining_turns,\n",
    "            is_t1_turn=env_state.is_t1_turn,\n",
    "        )\n",
    "\n",
    "    def _env_action_to_game_action(self, env_action: EnvAction) -> GameAction:\n",
    "        return GameAction(env_action.craftsman_id, *idxToActionEnum[env_action.action])\n",
    "\n",
    "    def _env_actions_to_game_actions(self, env_actions: chex.Array) -> chex.Array:\n",
    "        return [self._env_action_to_game_action(env_action) for env_action in env_actions]\n",
    "\n",
    "\n",
    "    def render(self, state: ProconState) -> Optional[NDArray]:\n",
    "        return self._viewer.render_with_score(state, self.game_options)\n",
    "\n",
    "    def animate(\n",
    "        self,\n",
    "        states: Sequence[ProconState],\n",
    "        interval: int = 200,\n",
    "        save_path: Optional[str] = None,\n",
    "    ) -> matplotlib.animation.FuncAnimation:\n",
    "        return self._viewer.animate_with_score(states, self.game_options, interval, save_path)\n",
    "\n",
    "# ENV_ID = \"Procon-v0\"\n",
    "# if not ENV_ID in _REGISTRY:\n",
    "#     register(id=\"Procon-v0\", entry_point=\"procon:ProconJumanji\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "action_path = \"test-cases/match-259.txt\"\n",
    "map_path = \"test-cases/map-259-game-2.txt\"\n",
    "\n",
    "from utils import load_map\n",
    "\n",
    "mapdata = load_map(map_path)\n",
    "map_formatted = [[map_tile_to_bitmask(x) for x in row] for row in mapdata['game_map']]\n",
    "\n",
    "\n",
    "counter = 0\n",
    "craftsman_strid_to_intid = {}\n",
    "craftsmen_formatted = []\n",
    "for c in mapdata['craftsmen']['team1']:\n",
    "    new_craftsman = Craftsman(counter, c['x'], c['y'], True)\n",
    "    craftsmen_formatted.append(new_craftsman)\n",
    "    craftsman_strid_to_intid[c['id']] = counter\n",
    "    counter += 1\n",
    "for c in mapdata['craftsmen']['team2']:\n",
    "    new_craftsman = Craftsman(counter, c['x'], c['y'], False)\n",
    "    craftsmen_formatted.append(new_craftsman)\n",
    "    craftsman_strid_to_intid[c['id']] = counter\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "actiontxt = open(action_path, \"r\").read()\n",
    "\n",
    "# Split the input string based on \"- -\"\n",
    "turns = actiontxt.strip().split(\"- -\")\n",
    "turns = [section.strip() for section in turns]\n",
    "\n",
    "# Convert each section to an array of arrays\n",
    "actions = [[action.split(' ') for action in (turn.split('\\n') if turn else [])] for turn in turns]\n",
    "actions = [[GameAction(craftsman_strid_to_intid[action[0]], *idxToActionEnum[int(action[1])]) for action in turn] for turn in actions]\n",
    "\n",
    "# %%\n",
    "go = GameOptions()\n",
    "go.mapWidth = mapdata['game_settings']['map_width']\n",
    "go.mapHeight = mapdata['game_settings']['map_height']\n",
    "go.maxTurns = mapdata['game_settings']['max_turn']\n",
    "go.wallCoeff = mapdata['score_coefficients']['wall']\n",
    "go.castleCoeff = mapdata['score_coefficients']['castle']\n",
    "go.territoryCoeff = mapdata['score_coefficients']['territory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ProconJumanji(go, map_formatted, craftsmen_formatted)\n",
    "\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "key1, key2 = jax.random.split(random_key)\n",
    "batch_size = 1\n",
    "\n",
    "keys = jax.random.split(key1, batch_size)\n",
    "state, timestep = env.reset(keys[0])\n",
    "\n",
    "def game_actions_to_env_actions(all_turns_actions):\n",
    "    env_all_turns_actions = []\n",
    "    for actions in all_turns_actions:\n",
    "        turn_actions = []\n",
    "        for action in actions:\n",
    "            turn_actions.append(EnvAction(action.subActionType.value, action.craftsmanId))\n",
    "        env_all_turns_actions.append(turn_actions)\n",
    "            \n",
    "    return env_all_turns_actions\n",
    "\n",
    "env_actions = game_actions_to_env_actions(actions)\n",
    "\n",
    "state_list = [state]\n",
    "\n",
    "# for i, actions_in_turn in enumerate(env_actions):\n",
    "#     # print(actions_in_turn)\n",
    "#     state, timestep = env.step(state, actions_in_turn)\n",
    "#     state_list.append(state)\n",
    "# env.animate(state_list, interval=200, save_path=f'records/test.gif')\n",
    "# env.render(state_list[-1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Actual AI\n",
    "## Import and Setup libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import required packages.\n",
    "\n",
    "from typing import Any, Callable, Dict, Sequence, Tuple\n",
    "from colorama import Fore, Style\n",
    "\n",
    "import optax\n",
    "from optax._src.base import OptState\n",
    "import chex\n",
    "import distrax\n",
    "import flax.linen as nn\n",
    "from flax import struct\n",
    "from flax.core.frozen_dict import FrozenDict\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# Env requirements\n",
    "import jumanji\n",
    "from jumanji.env import Environment\n",
    "from jumanji import specs\n",
    "from jumanji.wrappers import AutoResetWrapper\n",
    "\n",
    "# Mava Helpful functions and types\n",
    "from mava.utils.jax import merge_leading_dims\n",
    "from mava.wrappers.jumanji import (\n",
    "    AgentIDWrapper,\n",
    "    LogWrapper,\n",
    "    ObservationGlobalState,\n",
    "    RwareMultiAgentWithGlobalStateWrapper,\n",
    ")\n",
    "from mava.types import ExperimentOutput, LearnerState, OptStates, Params, PPOTransition\n",
    "from mava.evaluator import evaluator_setup\n",
    "\n",
    "# Plot requirements\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import time\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor Network.\"\"\"\n",
    "\n",
    "    action_dim: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, observation: ProconObservation) -> distrax.Categorical:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = observation.agents_view\n",
    "\n",
    "        actor_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "        actor_output = nn.relu(actor_output)\n",
    "        actor_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
    "            actor_output\n",
    "        )\n",
    "        actor_output = nn.relu(actor_output)\n",
    "        actor_output = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_output)\n",
    "\n",
    "        masked_logits = jnp.where(\n",
    "            observation.action_mask,\n",
    "            actor_output,\n",
    "            jnp.finfo(jnp.float32).min,\n",
    "        )\n",
    "        actor_policy = distrax.Categorical(logits=masked_logits)\n",
    "\n",
    "        return actor_policy\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic Network.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, observation: ProconObservation) -> chex.Array:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "\n",
    "        critic_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
    "            observation.global_state\n",
    "        )\n",
    "        critic_output = nn.relu(critic_output)\n",
    "        critic_output = nn.Dense(128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
    "            critic_output\n",
    "        )\n",
    "        critic_output = nn.relu(critic_output)\n",
    "        critic_output = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic_output\n",
    "        )\n",
    "\n",
    "        return jnp.squeeze(critic_output, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner_fn(\n",
    "    env: jumanji.Environment,\n",
    "    apply_fns: Tuple[Callable, Callable],\n",
    "    update_fns: Tuple[Callable, Callable],\n",
    "    config: Dict,\n",
    ") -> Callable:\n",
    "    \"\"\"Get the learner function.\"\"\"\n",
    "\n",
    "    # Unpack apply and update functions.\n",
    "    actor_apply_fn, critic_apply_fn = apply_fns\n",
    "    actor_update_fn, critic_update_fn = update_fns\n",
    "\n",
    "    def _update_step(learner_state: LearnerState, _: Any) -> Tuple[LearnerState, Tuple]:\n",
    "        \"\"\"A single update of the network.\n",
    "\n",
    "        This function steps the environment and records the trajectory batch for\n",
    "        training. It then calculates advantages and targets based on the recorded\n",
    "        trajectory and updates the actor and critic networks based on the calculated\n",
    "        losses.\n",
    "\n",
    "        Args:\n",
    "            learner_state (NamedTuple):\n",
    "                - params (Params): The current model parameters.\n",
    "                - opt_states (OptStates): The current optimizer states.\n",
    "                - rng (PRNGKey): The random number generator state.\n",
    "                - env_state (State): The environment state.\n",
    "                - last_timestep (TimeStep): The last timestep in the current trajectory.\n",
    "            _ (Any): The current metrics info.\n",
    "        \"\"\"\n",
    "\n",
    "        def _env_step(learner_state: LearnerState, _: Any) -> Tuple[LearnerState, PPOTransition]:\n",
    "            \"\"\"Step the environment.\"\"\"\n",
    "            params, opt_states, rng, env_state, last_timestep = learner_state\n",
    "\n",
    "            # SELECT ACTION\n",
    "            rng, policy_rng = jax.random.split(rng)\n",
    "            actor_policy = actor_apply_fn(params.actor_params, last_timestep.observation)\n",
    "            value = critic_apply_fn(params.critic_params, last_timestep.observation)\n",
    "            action = actor_policy.sample(seed=policy_rng)\n",
    "            log_prob = actor_policy.log_prob(action)\n",
    "\n",
    "            # STEP ENVIRONMENT\n",
    "            env_state, timestep = jax.vmap(env.step, in_axes=(0, 0))(env_state, action)\n",
    "\n",
    "            # LOG EPISODE METRICS\n",
    "            done, reward = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.repeat(x, config[\"num_agents\"]).reshape(config[\"num_envs\"], -1),\n",
    "                (timestep.last(), timestep.reward),\n",
    "            )\n",
    "            info = {\n",
    "                \"episode_return\": env_state.episode_return_info,\n",
    "                \"episode_length\": env_state.episode_length_info,\n",
    "            }\n",
    "\n",
    "            transition = PPOTransition(\n",
    "                done, action, value, reward, log_prob, last_timestep.observation, info\n",
    "            )\n",
    "            learner_state = LearnerState(params, opt_states, rng, env_state, timestep)\n",
    "            return learner_state, transition\n",
    "\n",
    "        # STEP ENVIRONMENT FOR ROLLOUT LENGTH\n",
    "        learner_state, traj_batch = jax.lax.scan(\n",
    "            _env_step, learner_state, None, config[\"rollout_length\"]\n",
    "        )\n",
    "\n",
    "        # CALCULATE ADVANTAGE\n",
    "        params, opt_states, rng, env_state, last_timestep = learner_state\n",
    "        last_val = critic_apply_fn(params.critic_params, last_timestep.observation)\n",
    "\n",
    "        def _calculate_gae(\n",
    "            traj_batch: PPOTransition, last_val: chex.Array\n",
    "        ) -> Tuple[chex.Array, chex.Array]:\n",
    "            \"\"\"Calculate the GAE.\"\"\"\n",
    "\n",
    "            def _get_advantages(gae_and_next_value: Tuple, transition: PPOTransition) -> Tuple:\n",
    "                \"\"\"Calculate the GAE for a single transition.\"\"\"\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward = (\n",
    "                    transition.done,\n",
    "                    transition.value,\n",
    "                    transition.reward,\n",
    "                )\n",
    "                delta = reward + config[\"gamma\"] * next_value * (1 - done) - value\n",
    "                gae = delta + config[\"gamma\"] * config[\"gae_lambda\"] * (1 - done) * gae\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return advantages, advantages + traj_batch.value\n",
    "\n",
    "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "        def _update_epoch(update_state: Tuple, _: Any) -> Tuple:\n",
    "            \"\"\"Update the network for a single epoch.\"\"\"\n",
    "\n",
    "            def _update_minibatch(train_state: Tuple, batch_info: Tuple) -> Tuple:\n",
    "                \"\"\"Update the network for a single minibatch.\"\"\"\n",
    "\n",
    "                # UNPACK TRAIN STATE AND BATCH INFO\n",
    "                params, opt_states = train_state\n",
    "                traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                def _actor_loss_fn(\n",
    "                    actor_params: FrozenDict,\n",
    "                    actor_opt_state: OptState,\n",
    "                    traj_batch: PPOTransition,\n",
    "                    gae: chex.Array,\n",
    "                ) -> Tuple:\n",
    "                    \"\"\"Calculate the actor loss.\"\"\"\n",
    "                    # RERUN NETWORK\n",
    "                    actor_policy = actor_apply_fn(actor_params, traj_batch.obs)\n",
    "                    log_prob = actor_policy.log_prob(traj_batch.action)\n",
    "\n",
    "                    # CALCULATE ACTOR LOSS\n",
    "                    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    loss_actor1 = ratio * gae\n",
    "                    loss_actor2 = (\n",
    "                        jnp.clip(\n",
    "                            ratio,\n",
    "                            1.0 - config[\"clip_eps\"],\n",
    "                            1.0 + config[\"clip_eps\"],\n",
    "                        )\n",
    "                        * gae\n",
    "                    )\n",
    "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                    loss_actor = loss_actor.mean()\n",
    "                    entropy = actor_policy.entropy().mean()\n",
    "\n",
    "                    total_loss_actor = loss_actor - config[\"ent_coef\"] * entropy\n",
    "                    return total_loss_actor, (loss_actor, entropy)\n",
    "\n",
    "                def _critic_loss_fn(\n",
    "                    critic_params: FrozenDict,\n",
    "                    critic_opt_state: OptState,\n",
    "                    traj_batch: PPOTransition,\n",
    "                    targets: chex.Array,\n",
    "                ) -> Tuple:\n",
    "                    \"\"\"Calculate the critic loss.\"\"\"\n",
    "                    # RERUN NETWORK\n",
    "                    value = critic_apply_fn(critic_params, traj_batch.obs)\n",
    "\n",
    "                    # CALCULATE VALUE LOSS\n",
    "                    value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
    "                        -config[\"clip_eps\"], config[\"clip_eps\"]\n",
    "                    )\n",
    "                    value_losses = jnp.square(value - targets)\n",
    "                    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "                    critic_total_loss = config[\"vf_coef\"] * value_loss\n",
    "                    return critic_total_loss, (value_loss)\n",
    "\n",
    "                # CALCULATE ACTOR LOSS\n",
    "                actor_grad_fn = jax.value_and_grad(_actor_loss_fn, has_aux=True)\n",
    "                actor_loss_info, actor_grads = actor_grad_fn(\n",
    "                    params.actor_params, opt_states.actor_opt_state, traj_batch, advantages\n",
    "                )\n",
    "\n",
    "                # CALCULATE CRITIC LOSS\n",
    "                critic_grad_fn = jax.value_and_grad(_critic_loss_fn, has_aux=True)\n",
    "                critic_loss_info, critic_grads = critic_grad_fn(\n",
    "                    params.critic_params, opt_states.critic_opt_state, traj_batch, targets\n",
    "                )\n",
    "\n",
    "                # Compute the parallel mean (pmean) over the batch.\n",
    "                # This calculation is inspired by the Anakin architecture demo notebook.\n",
    "                # available at https://tinyurl.com/26tdzs5x\n",
    "                # This pmean could be a regular mean as the batch axis is on the same device.\n",
    "                actor_grads, actor_loss_info = jax.lax.pmean(\n",
    "                    (actor_grads, actor_loss_info), axis_name=\"batch\"\n",
    "                )\n",
    "                # pmean over devices.\n",
    "                actor_grads, actor_loss_info = jax.lax.pmean(\n",
    "                    (actor_grads, actor_loss_info), axis_name=\"device\"\n",
    "                )\n",
    "\n",
    "                critic_grads, critic_loss_info = jax.lax.pmean(\n",
    "                    (critic_grads, critic_loss_info), axis_name=\"batch\"\n",
    "                )\n",
    "                # pmean over devices.\n",
    "                critic_grads, critic_loss_info = jax.lax.pmean(\n",
    "                    (critic_grads, critic_loss_info), axis_name=\"device\"\n",
    "                )\n",
    "\n",
    "                # UPDATE ACTOR PARAMS AND OPTIMISER STATE\n",
    "                actor_updates, actor_new_opt_state = actor_update_fn(\n",
    "                    actor_grads, opt_states.actor_opt_state\n",
    "                )\n",
    "                actor_new_params = optax.apply_updates(params.actor_params, actor_updates)\n",
    "\n",
    "                # UPDATE CRITIC PARAMS AND OPTIMISER STATE\n",
    "                critic_updates, critic_new_opt_state = critic_update_fn(\n",
    "                    critic_grads, opt_states.critic_opt_state\n",
    "                )\n",
    "                critic_new_params = optax.apply_updates(params.critic_params, critic_updates)\n",
    "\n",
    "                new_params = Params(actor_new_params, critic_new_params)\n",
    "                new_opt_state = OptStates(actor_new_opt_state, critic_new_opt_state)\n",
    "\n",
    "                # PACK LOSS INFO\n",
    "                total_loss = actor_loss_info[0] + critic_loss_info[0]\n",
    "                value_loss = critic_loss_info[1]\n",
    "                actor_loss = actor_loss_info[1][0]\n",
    "                entropy = actor_loss_info[1][1]\n",
    "                loss_info = (\n",
    "                    total_loss,\n",
    "                    (value_loss, actor_loss, entropy),\n",
    "                )\n",
    "\n",
    "                return (new_params, new_opt_state), loss_info\n",
    "\n",
    "            params, opt_states, traj_batch, advantages, targets, rng = update_state\n",
    "            rng, shuffle_rng = jax.random.split(rng)\n",
    "\n",
    "            # SHUFFLE MINIBATCHES\n",
    "            batch_size = config[\"rollout_length\"] * config[\"num_envs\"]\n",
    "            permutation = jax.random.permutation(shuffle_rng, batch_size)\n",
    "            batch = (traj_batch, advantages, targets)\n",
    "            batch = jax.tree_util.tree_map(lambda x: merge_leading_dims(x, 2), batch)\n",
    "            shuffled_batch = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "            )\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.reshape(x, [config[\"num_minibatches\"], -1] + list(x.shape[1:])),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "\n",
    "            # UPDATE MINIBATCHES\n",
    "            (params, opt_states), loss_info = jax.lax.scan(\n",
    "                _update_minibatch, (params, opt_states), minibatches\n",
    "            )\n",
    "\n",
    "            update_state = (params, opt_states, traj_batch, advantages, targets, rng)\n",
    "            return update_state, loss_info\n",
    "\n",
    "        update_state = (params, opt_states, traj_batch, advantages, targets, rng)\n",
    "\n",
    "        # UPDATE EPOCHS\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, config[\"ppo_epochs\"]\n",
    "        )\n",
    "\n",
    "        params, opt_states, traj_batch, advantages, targets, rng = update_state\n",
    "        learner_state = LearnerState(params, opt_states, rng, env_state, last_timestep)\n",
    "        metric = traj_batch.info\n",
    "        return learner_state, (metric, loss_info)\n",
    "\n",
    "    def learner_fn(learner_state: LearnerState) -> ExperimentOutput:\n",
    "        \"\"\"Learner function.\n",
    "\n",
    "        This function represents the learner, it updates the network parameters\n",
    "        by iteratively applying the `_update_step` function for a fixed number of\n",
    "        updates. The `_update_step` function is vectorized over a batch of inputs.\n",
    "\n",
    "        Args:\n",
    "            learner_state (NamedTuple):\n",
    "                - params (Params): The initial model parameters.\n",
    "                - opt_states (OptStates): The initial optimizer states.\n",
    "                - rng (chex.PRNGKey): The random number generator state.\n",
    "                - env_state (LogEnvState): The environment state.\n",
    "                - timesteps (TimeStep): The initial timestep in the initial trajectory.\n",
    "        \"\"\"\n",
    "\n",
    "        batched_update_step = jax.vmap(_update_step, in_axes=(0, None), axis_name=\"batch\")\n",
    "\n",
    "        learner_state, (metric, loss_info) = jax.lax.scan(\n",
    "            batched_update_step, learner_state, None, config[\"num_updates_per_eval\"]\n",
    "        )\n",
    "        total_loss, (value_loss, loss_actor, entropy) = loss_info\n",
    "        return ExperimentOutput(\n",
    "            learner_state=learner_state,\n",
    "            episodes_info=metric,\n",
    "            total_loss=total_loss,\n",
    "            value_loss=value_loss,\n",
    "            loss_actor=loss_actor,\n",
    "            entropy=entropy,\n",
    "        )\n",
    "\n",
    "    return learner_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learner_setup(\n",
    "    env: Environment, rngs: chex.Array, config: Dict\n",
    ") -> Tuple[Callable, Actor, LearnerState]:\n",
    "    \"\"\"Initialise learner_fn, network, optimiser, environment and states.\"\"\"\n",
    "    # Get available TPU cores.\n",
    "    n_devices = len(jax.devices())\n",
    "\n",
    "    # Get number of actions and agents.\n",
    "    num_actions = int(env.action_spec().num_values[0])\n",
    "    num_agents = env.action_spec().shape[0]\n",
    "    config[\"num_agents\"] = num_agents\n",
    "\n",
    "    # PRNG keys.\n",
    "    rng, rng_p = rngs\n",
    "\n",
    "    # Define network and optimiser.\n",
    "    actor_network = Actor(num_actions)\n",
    "    critic_network = Critic()\n",
    "    actor_optim = optax.chain(\n",
    "        optax.clip_by_global_norm(config[\"max_grad_norm\"]),\n",
    "        optax.adam(config[\"actor_lr\"], eps=1e-5),\n",
    "    )\n",
    "    critic_optim = optax.chain(\n",
    "        optax.clip_by_global_norm(config[\"max_grad_norm\"]),\n",
    "        optax.adam(config[\"critic_lr\"], eps=1e-5),\n",
    "    )\n",
    "\n",
    "    # Initialise observation.\n",
    "    obs = env.observation_spec().generate_value()\n",
    "    # Select only obs for a single agent.\n",
    "    init_x = ObservationGlobalState(\n",
    "        agents_view=obs.agents_view[0],\n",
    "        action_mask=obs.action_mask[0],\n",
    "        global_state=obs.global_state[0],\n",
    "        step_count=obs.step_count[0],\n",
    "    )\n",
    "    init_x = jax.tree_util.tree_map(lambda x: x[None, ...], init_x)\n",
    "\n",
    "    # Initialise actor params and optimiser state.\n",
    "    actor_params = actor_network.init(rng_p, init_x)\n",
    "    actor_opt_state = actor_optim.init(actor_params)\n",
    "\n",
    "    # Initialise critic params and optimiser state.\n",
    "    critic_params = critic_network.init(rng_p, init_x)\n",
    "    critic_opt_state = critic_optim.init(critic_params)\n",
    "\n",
    "    # Vmap network apply function over number of agents.\n",
    "    vmapped_actor_network_apply_fn = jax.vmap(\n",
    "        actor_network.apply,\n",
    "        in_axes=(None, 1),\n",
    "        out_axes=(1),\n",
    "    )\n",
    "    vmapped_critic_network_apply_fn = jax.vmap(\n",
    "        critic_network.apply,\n",
    "        in_axes=(None, 1),\n",
    "        out_axes=(1),\n",
    "    )\n",
    "\n",
    "    # Pack apply and update functions.\n",
    "    apply_fns = (vmapped_actor_network_apply_fn, vmapped_critic_network_apply_fn)\n",
    "    update_fns = (actor_optim.update, critic_optim.update)\n",
    "\n",
    "    # Get batched iterated update and replicate it to pmap it over cores.\n",
    "    learn = get_learner_fn(env, apply_fns, update_fns, config)\n",
    "    learn = jax.pmap(learn, axis_name=\"device\")\n",
    "\n",
    "    # Broadcast params and optimiser state to cores and batch.\n",
    "    broadcast = lambda x: jnp.broadcast_to(x, (n_devices, config[\"update_batch_size\"]) + x.shape)\n",
    "    actor_params = jax.tree_map(broadcast, actor_params)\n",
    "    actor_opt_state = jax.tree_map(broadcast, actor_opt_state)\n",
    "    critic_params = jax.tree_map(broadcast, critic_params)\n",
    "    critic_opt_state = jax.tree_map(broadcast, critic_opt_state)\n",
    "\n",
    "    # Initialise environment states and timesteps.\n",
    "    rng, *env_rngs = jax.random.split(\n",
    "        rng, n_devices * config[\"update_batch_size\"] * config[\"num_envs\"] + 1\n",
    "    )\n",
    "    env_states, timesteps = jax.vmap(env.reset, in_axes=(0))(\n",
    "        jnp.stack(env_rngs),\n",
    "    )\n",
    "\n",
    "    # Split rngs for each core.\n",
    "    rng, *step_rngs = jax.random.split(rng, n_devices * config[\"update_batch_size\"] + 1)\n",
    "\n",
    "    # Add dimension to pmap over.\n",
    "    reshape_step_rngs = lambda x: x.reshape((n_devices, config[\"update_batch_size\"]) + x.shape[1:])\n",
    "    step_rngs = reshape_step_rngs(jnp.stack(step_rngs))\n",
    "    reshape_states = lambda x: x.reshape(\n",
    "        (n_devices, config[\"update_batch_size\"], config[\"num_envs\"]) + x.shape[1:]\n",
    "    )\n",
    "    env_states = jax.tree_util.tree_map(reshape_states, env_states)\n",
    "    timesteps = jax.tree_util.tree_map(reshape_states, timesteps)\n",
    "\n",
    "    params = Params(actor_params, critic_params)\n",
    "    opt_states = OptStates(actor_opt_state, critic_opt_state)\n",
    "\n",
    "    init_learner_state = LearnerState(params, opt_states, step_rngs, env_states, timesteps)\n",
    "    return learn, actor_network, init_learner_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"actor_lr\": 2.5e-4,\n",
    "    \"critic_lr\": 2.5e-4,\n",
    "    \"update_batch_size\": 2,\n",
    "    \"rollout_length\": 128,\n",
    "    \"num_updates\": 150,\n",
    "    \"num_envs\": 512,\n",
    "    \"ppo_epochs\": 16,\n",
    "    \"num_minibatches\": 32,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_eps\": 0.2,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"env_name\": \"Procon-v0\",\n",
    "    \"num_eval_episodes\": 32,\n",
    "    \"num_evaluation\": 50,\n",
    "    \"evaluation_greedy\": False,\n",
    "    \"add_agent_id\": True,\n",
    "    \"seed\":42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ProconJumanji(go, map_formatted, craftsmen_formatted)\n",
    "# env = RwareMultiAgentWithGlobalStateWrapper(env)\n",
    "# if config[\"add_agent_id\"]:\n",
    "#     env = AgentIDWrapper(env=env, has_global_state=True)\n",
    "env = AutoResetWrapper(env)\n",
    "env = LogWrapper(env)\n",
    "eval_env = ProconJumanji(go, map_formatted, craftsmen_formatted)\n",
    "# eval_env = RwareMultiAgentWithGlobalStateWrapper(eval_env)\n",
    "# if config[\"add_agent_id\"]:\n",
    "#     eval_env = AgentIDWrapper(env=eval_env, has_global_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only length-1 arrays can be converted to Python scalars.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb Cell 20\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m rng, rng_e, rng_p \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mPRNGKey(config[\u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m]), num\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Setup learner.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m learn, actor_network, learner_state \u001b[39m=\u001b[39m learner_setup(env, (rng, rng_p), config)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Setup evaluator.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m evaluator, absolute_metric_evaluator, (trained_params, eval_rngs) \u001b[39m=\u001b[39m evaluator_setup(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         eval_env\u001b[39m=\u001b[39meval_env,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         rng_e\u001b[39m=\u001b[39mrng_e,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         config\u001b[39m=\u001b[39mconfig,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     )\n",
      "\u001b[1;32m/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb Cell 20\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m n_devices \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(jax\u001b[39m.\u001b[39mdevices())\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Get number of actions and agents.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m num_actions \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(env\u001b[39m.\u001b[39;49maction_spec()\u001b[39m.\u001b[39;49mnum_values[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m num_agents \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_spec()\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/f/procon-2023/cpp-simulator/Procon2023/procon-jumanji.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m config[\u001b[39m\"\u001b[39m\u001b[39mnum_agents\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_agents\n",
      "File \u001b[0;32m~/mambaforge/envs/procon-ai/lib/python3.9/site-packages/jax/_src/array.py:269\u001b[0m, in \u001b[0;36mArrayImpl.__int__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__int__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 269\u001b[0m   core\u001b[39m.\u001b[39;49mcheck_scalar_conversion(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    270\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\u001b[39m.\u001b[39m\u001b[39m__int__\u001b[39m()\n",
      "File \u001b[0;32m~/mambaforge/envs/procon-ai/lib/python3.9/site-packages/jax/_src/core.py:602\u001b[0m, in \u001b[0;36mcheck_scalar_conversion\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_scalar_conversion\u001b[39m(arr: Array):\n\u001b[1;32m    601\u001b[0m   \u001b[39mif\u001b[39;00m arr\u001b[39m.\u001b[39msize \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOnly length-1 arrays can be converted to Python scalars.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    603\u001b[0m   \u001b[39mif\u001b[39;00m arr\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m ():\n\u001b[1;32m    604\u001b[0m     \u001b[39m# Added 2023 September 18.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mConversion of an array with ndim > 0 to a scalar is deprecated, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mand will error in future.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Only length-1 arrays can be converted to Python scalars."
     ]
    }
   ],
   "source": [
    "# PRNG keys.\n",
    "rng, rng_e, rng_p = jax.random.split(jax.random.PRNGKey(config[\"seed\"]), num=3)\n",
    "\n",
    "# Setup learner.\n",
    "learn, actor_network, learner_state = learner_setup(env, (rng, rng_p), config)\n",
    "\n",
    "# Setup evaluator.\n",
    "evaluator, absolute_metric_evaluator, (trained_params, eval_rngs) = evaluator_setup(\n",
    "        eval_env=eval_env,\n",
    "        rng_e=rng_e,\n",
    "        network=actor_network,\n",
    "        params=learner_state.params.actor_params,\n",
    "        config=config,\n",
    "    )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate total timesteps.\n",
    "n_devices = len(jax.devices())\n",
    "config[\"num_updates_per_eval\"] = config[\"num_updates\"] // config[\"num_evaluation\"]\n",
    "steps_per_rollout = (\n",
    "    n_devices\n",
    "    * config[\"num_updates_per_eval\"]\n",
    "    * config[\"rollout_length\"]\n",
    "    * config[\"update_batch_size\"]\n",
    "    * config[\"num_envs\"]\n",
    ")\n",
    "\n",
    "# Run experiment for a total number of evaluations.\n",
    "ep_returns=[]\n",
    "     \n",
    "\n",
    "\n",
    "learn(learner_state)\n",
    "\n",
    "# Compile the evaluator function\n",
    "_ = evaluator(trained_params, eval_rngs)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "for i in range(config[\"num_evaluation\"]):\n",
    "    # Train.\n",
    "    with TimeIt(\"EXECUTION\",environment_steps=steps_per_rollout,):\n",
    "        learner_output = learn(learner_state)\n",
    "        jax.block_until_ready(learner_output)\n",
    "\n",
    "\n",
    "    # Prepare for evaluation.\n",
    "    trained_params = jax.tree_util.tree_map(\n",
    "            lambda x: x[:, 0, ...],\n",
    "            learner_output.learner_state.params.actor_params,  # Select only actor params\n",
    "    )\n",
    "    rng_e, *eval_rngs = jax.random.split(rng_e, n_devices + 1)\n",
    "    eval_rngs = jnp.stack(eval_rngs)\n",
    "    eval_rngs = eval_rngs.reshape(n_devices, -1)\n",
    "\n",
    "    # Evaluate.\n",
    "    evaluator_output = evaluator(trained_params, eval_rngs)\n",
    "    jax.block_until_ready(evaluator_output)\n",
    "    ep_returns=plot_performance(evaluator_output, ep_returns, start_time)\n",
    "\n",
    "    # Update runner state to continue training.\n",
    "    learner_state = learner_output.learner_state\n",
    "\n",
    "# Return trained params to be used for rendering or testing.\n",
    "trained_params= jax.tree_util.tree_map(\n",
    "    lambda x: x[0, 0, ...], learner_output.learner_state.params.actor_params\n",
    ")\n",
    "print(f\"{Fore.CYAN}{Style.BRIGHT}MAPPO experiment completed{Style.RESET_ALL}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procon-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
